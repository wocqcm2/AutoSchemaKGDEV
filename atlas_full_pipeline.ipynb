{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc524e",
   "metadata": {},
   "source": [
    "# Networkx ATLAS KG construction and RAG example\n",
    "This notebook demonstrates the full streamlined process of creating a knowledge graph (KG) using the atlas-rag package and performing retrieval-augmented generation (RAG) with our created RAG methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a035b3",
   "metadata": {},
   "source": [
    "## ATLAS KG Construction\n",
    "It is suggested to use local hf model to run the KG construction code, as llm api service provider use optimized, lightweight models to reduce costs, which may sacrifice performance, and hence hard to have guaranteed performance. (for example from fp16 to fp8 etc.)\n",
    "\n",
    "ATLAS KG construction consist of 5 steps:\n",
    "- Triples Json Generation (Base KG Json)\n",
    "- Convert Triples Json to Triples csv\n",
    "- Conceptualize Entity in Triples csv\n",
    "- Merge Concept CSV to Triples CSV\n",
    "- Convert CSV to graphml for networkx to perform rag / to neo4j dumps for Billion KG RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor\n",
    "from atlas_rag.kg_construction.triple_config import ProcessingConfig\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# client = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "filename_pattern = 'CICGPC_Glazing_ver1.0a'\n",
    "output_directory = f'import/{filename_pattern}'\n",
    "triple_generator = LLMGenerator(client, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37353f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extraction_config = ProcessingConfig(\n",
    "      model_path=model_name,\n",
    "      data_directory=\"example_data\",\n",
    "      filename_pattern=filename_pattern,\n",
    "      batch_size_triple=3,\n",
    "      batch_size_concept=16,\n",
    "      output_directory=f\"{output_directory}\",\n",
    "      max_new_tokens=2048,\n",
    "      max_workers=3,\n",
    "      remove_doc_spaces=True, # For removing duplicated spaces in the document text\n",
    ")\n",
    "kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c248a3",
   "metadata": {},
   "source": [
    "### Triples Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct entity&event graph\n",
    "kg_extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Triples Json to CSV\n",
    "kg_extractor.convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335211ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Generation\n",
    "kg_extractor.generate_concept_csv_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5823e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor.create_concept_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480f15",
   "metadata": {},
   "source": [
    "# Choice 1: Convert to graphml for networkx rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to graphml for networkx\n",
    "kg_extractor.convert_to_graphml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f197d",
   "metadata": {},
   "source": [
    "## ATLAS Multihop QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b957c1",
   "metadata": {},
   "source": [
    "In order to perform RAG, one need to first create embeddings & faiss index for constructed KG\n",
    "\n",
    "[There maybe performance difference in using AutoModel and Sentence Transformer for NV-Ebmed-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import NvEmbed, SentenceEmbedding\n",
    "from transformers import AutoModel\n",
    "# Load the SentenceTransformer model\n",
    "encoder_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sentence_model = SentenceTransformer(encoder_model_name, trust_remote_code=True, model_kwargs={'device_map': \"auto\"})\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)\n",
    "# sentence_model.max_seq_length = 32768\n",
    "# sentence_model.tokenizer.padding_side=\"right\"\n",
    "# sentence_model = AutoModel.from_pretrained(encoder_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "# sentence_encoder = NvEmbed(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a106e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# reader_model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "reader_model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  # base_url=\"https://openrouter.ai/api/v1\",\n",
    "  # api_key=config['settings']['OPENROUTER_API_KEY'],\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "llm_generator = LLMGenerator(client=client, model_name=reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag.vectorstore import create_embeddings_and_index\n",
    "keyword = 'CICGPC_Glazing_ver1.0a'\n",
    "working_directory = f'import/{keyword}'\n",
    "data = create_embeddings_and_index(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    model_name = encoder_model_name,\n",
    "    working_directory=working_directory,\n",
    "    keyword=keyword,\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    normalize_embeddings= True,\n",
    "    text_batch_size=64,\n",
    "    node_and_edge_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize desired RAG method for benchmarking\n",
    "from atlas_rag.retriever import HippoRAG2Retriever\n",
    "from atlas_rag import setup_logger\n",
    "\n",
    "hipporag2_retriever = HippoRAG2Retriever(\n",
    "    llm_generator=llm_generator,\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    data = data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform retrieval\n",
    "content, sorted_context_ids = hipporag2_retriever.retrieve(\"How is the U-value relevant to thermal insulation performance in glazing products?\", topN=3)\n",
    "print(f\"Retrieved content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b515ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start benchmarking\n",
    "sorted_context = \"\\n\".join(content)\n",
    "llm_generator.generate_with_context(\"How is the U-value relevant to thermal insulation performance in glazing products?\", sorted_context, max_new_tokens=2048, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a428",
   "metadata": {},
   "source": [
    "# Choice 2: Convert to neo4j dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d114689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import SentenceEmbedding\n",
    "# use sentence embedding if you want to use sentence transformer\n",
    "# use NvEmbed if you want to use NvEmbed-v2 model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numeric id to the csv so that we can use vector indices\n",
    "kg_extractor.add_numeric_id()\n",
    "\n",
    "# compute embedding\n",
    "kg_extractor.compute_kg_embedding(sentence_encoder) # default encoder_model_name=\"all-MiniLM-L12-v2\", only compute all embeddings except any concept related embeddings\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"all-MiniLM-L12-v2\")\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"nvidia/NV-Embed-v2\")\n",
    "\n",
    "# create faiss index\n",
    "kg_extractor.create_faiss_index() # default index_type=\"HNSW,Flat\", other options: \"IVF65536_HNSW32,Flat\" for large KG\n",
    "# kg_extractor.create_faiss_index(index_type=\"HNSW,Flat\")\n",
    "# kg_extractor.create_faiss_index(index_type=\"IVF65536_HNSW32,Flat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d518",
   "metadata": {},
   "source": [
    "## Install Neo4j Server\n",
    "\n",
    "Go to the AutoschemaKG/neo4j_scripts directory\n",
    "\n",
    "```sh get_neo4j_demo.sh```\n",
    "\n",
    "Then there a neo4j server is install in the directory: neo4j-server-dulce\n",
    "\n",
    "Start the newly instealled empty Neo4j server for testing\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "\n",
    "## Config Neo4j Server\n",
    "\n",
    "Stop the server first before config and import data\n",
    "\n",
    "```sh stop_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "Copy the ```AutoschemaKG/neo4j_scripts/neo4j.conf``` file to the conf directory of the Neo4j server (```neo4j-server-dulce/conf```). Then, update the following settings as needed: 1.Set dbms.default_database to the desired dataset name, such as ```wiki-csv-json-text```, ```pes2o-csv-json-text```, or ```cc-csv-json-text```. In this case we make it ```dulce-csv-json-text``` 2.Configure the Bolt, HTTP, and HTTPS connectors according to your requirements.\n",
    "\n",
    "I have set up the config port to some random ports to avoid port conflicts in ```neo4j-server-dulce/conf/neo4j.conf``` .\n",
    "\n",
    " \n",
    "``` \n",
    "# Bolt connector\n",
    "server.bolt.enabled=true\n",
    "#server.bolt.tls_level=DISABLED\n",
    "server.bolt.listen_address=0.0.0.0:8612\n",
    "server.bolt.advertised_address=:8612\n",
    "\n",
    "# HTTP Connector. There can be zero or one HTTP connectors.\n",
    "server.http.enabled=true\n",
    "server.http.listen_address=0.0.0.0:7612\n",
    "server.http.advertised_address=:7612\n",
    "\n",
    "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
    "server.https.enabled=false\n",
    "server.https.listen_address=0.0.0.0:7781\n",
    "server.https.advertised_address=:7781\n",
    "```\n",
    "\n",
    "\n",
    "## Import Data\n",
    "We use the admin import method to import data, which is the fastest way. Other methods are too slow for large graphs.\n",
    "\n",
    "\n",
    "## Load the CSV files into Neo4j\n",
    "\n",
    "We try to import data from previously constructed csv files with numeric ids. All the csv files are in ```import/Dulce```. \n",
    "In total six csv files for the nodes and edges of triples, text chunks, and concepts. \n",
    "\n",
    "``` shell\n",
    "./neo4j-server-dulce/bin/neo4j-admin database import full dulce-csv-json-text \\\n",
    "    --nodes ./import/Dulce/triples_csv/triple_nodes_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/triples_csv/text_nodes_Dulce_from_json_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/concept_csv/concept_nodes_Dulce_from_json_with_concept.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/triple_edges_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/text_edges_Dulce_from_json.csv \\\n",
    "    --relationships ./import/Dulce/concept_csv/concept_edges_Dulce_from_json_with_concept.csv  \\\n",
    "    --overwrite-destination \\\n",
    "    --multiline-fields=true \\\n",
    "    --id-type=string \\\n",
    "    --verbose --skip-bad-relationships=true\n",
    "```\n",
    "\n",
    "When this is finished, you can see the following notifications\n",
    "\n",
    "```shell\n",
    "IMPORT DONE in 2s 475ms. \n",
    "Imported:\n",
    "  1183 nodes\n",
    "  2519 relationships\n",
    "  6743 properties\n",
    "Peak memory usage: 1.032GiB\n",
    "```\n",
    "\n",
    "Then you can start host it by running in ```./neo4j_scripts```\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "When you see the following line, then it is working well.\n",
    "\n",
    "\n",
    "```Started neo4j (pid:742490). It is available at http://0.0.0.0:7612```\n",
    "\n",
    "\n",
    "\n",
    "If you want to use the python driver to run neo4j, you need to use port 8612. You can access http://0.0.0.0:7612 in browser as well to use the neo4j GUI. \n",
    "\n",
    "The default user is ```neo4j``` with password ```admin2024```. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e827",
   "metadata": {},
   "source": [
    "## ATLAS Billion Level RAG\n",
    "The LargeKGRetriever is designed to perform retrieval on a billion-level graph. \n",
    "\n",
    "There is a trade-off between retrieval performance and speed; this serves as a proof of concept for a billion-level knowledge graph.\n",
    "\n",
    "After successfully hosting the Neo4j database, you can run the provided Python script to host the RAG API:\n",
    "```shell\n",
    "python neo4j_api_host/atlas_api_demo.py \n",
    "```\n",
    "\n",
    "During the first startup of the API, it will create the necessary indexes and projection graphs in the Neo4j database for faster queries and computations. The time required for this process may vary depending on the size of the database. You can monitor the creation of these items in http://localhost:7612 by using the following commands:\n",
    "\n",
    "To view the projected graphs:\n",
    "```cypher\n",
    "CALL gds.graph.list()\n",
    "```\n",
    "To view the indexes:\n",
    "```cypher\n",
    "SHOW INDEXES\n",
    "```\n",
    "\n",
    "The projected graph will be deleted after the database is shut down, while the indexes will not be removed.\n",
    "\n",
    "After you saw: \\\n",
    "Index NodeNumericIDIndex created in 0.09 seconds \\\n",
    "Index TextNumericIDIndex created in 0.11 seconds \\\n",
    "Index EntityEventEdgeNumericIDIndex created in 0.02 seconds \\\n",
    "Projection graph largekgrag_graph created in 5.42 seconds \n",
    "\n",
    "You can perform rag as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a02dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url =\"http://0.0.0.0:10085/v1/\"\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "# knowledge graph en_simple_wiki_v0\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that answers questions based on the knowledge graph.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: Who is Alex Mercer?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama\",\n",
    "    messages=message,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-rag-gpu-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
