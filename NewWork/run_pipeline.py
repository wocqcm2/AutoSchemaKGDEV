#!/usr/bin/env python3
"""
Direct Concept Pipeline Runner
ç®€å•çš„è¿è¡Œè„šæœ¬
"""

import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append('..')

def main():
    parser = argparse.ArgumentParser(description='Run Direct Concept Extraction Pipeline')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--data_dir', required=True, help='Data directory path')
    parser.add_argument('--filename_pattern', required=True, help='Filename pattern to match')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', default='gpt-4o', help='Model name (default: gpt-4o)')
    parser.add_argument('--api_key', help='OpenAI API key (or set OPENAI_API_KEY env var)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='NewWork/output', help='Output directory')
    parser.add_argument('--output_name', help='Custom output name')
    
    # å¤„ç†å‚æ•°
    parser.add_argument('--mode', choices=['passage_concept', 'hierarchical_concept'], 
                       default='passage_concept', help='Extraction mode')
    parser.add_argument('--language', choices=['en', 'zh'], default='en', help='Language')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for processing')
    parser.add_argument('--chunk_size', type=int, default=1024, help='Text chunk size')
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--extract_only', action='store_true', help='Only extract concepts')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    
    args = parser.parse_args()
    
    # è®¾ç½®API key
    if args.api_key:
        os.environ['OPENAI_API_KEY'] = args.api_key
    elif not os.getenv('OPENAI_API_KEY'):
        print("âŒ Error: Please provide --api_key or set OPENAI_API_KEY environment variable")
        return 1
    
    try:
        # å¯¼å…¥ä¾èµ–
        from openai import OpenAI
        from atlas_rag.llm_generator import LLMGenerator
        from direct_concept_pipeline import DirectConceptPipeline, create_default_config
        
        print(f"ğŸš€ Starting Direct Concept Pipeline")
        print(f"ğŸ“ Data directory: {args.data_dir}")
        print(f"ğŸ” Filename pattern: {args.filename_pattern}")
        print(f"ğŸ¤– Model: {args.model}")
        print(f"ğŸŒ Language: {args.language}")
        print(f"âš™ï¸ Mode: {args.mode}")
        
        # è®¾ç½®æ¨¡å‹
        client = OpenAI()
        model = LLMGenerator(client, model_name=args.model)
        
        # åˆ›å»ºé…ç½®
        config = create_default_config(
            model_path=args.model,
            data_directory=args.data_dir,
            filename_pattern=args.filename_pattern,
            output_directory=args.output_dir,
            extraction_mode=args.mode,
            language=args.language
        )
        
        # æ›´æ–°é…ç½®
        config.batch_size_concept = args.batch_size
        config.text_chunk_size = args.chunk_size
        config.debug_mode = args.debug
        
        # åˆ›å»ºpipeline
        pipeline = DirectConceptPipeline(model, config)
        
        # è¿è¡Œpipeline
        if args.extract_only:
            print("\nğŸ”„ Running concept extraction only...")
            outputs = pipeline.run_extraction_only()
        else:
            print("\nğŸ”„ Running full pipeline...")
            outputs = pipeline.run_full_pipeline(args.output_name)
        
        print("\nâœ… Pipeline completed successfully!")
        print("\nğŸ“ Generated files:")
        for key, path in outputs.items():
            print(f"   ğŸ“„ {key}: {path}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Pipeline interrupted by user")
        return 1
    except Exception as e:
        print(f"\nâŒ Pipeline failed: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main()) 