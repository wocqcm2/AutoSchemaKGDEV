#!/usr/bin/env python3
"""
Configuration Loader for NewWork Pipeline
ä»config.jsonåŠ è½½é…ç½®çš„å·¥å…·ç±»
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass

from direct_concept_config import DirectConceptConfig


@dataclass
class ApiConfig:
    """APIé…ç½®ç±»"""
    base_url: str
    api_key: str
    model: str
    timeout: int = 120
    max_retries: int = 3


class ConfigLoader:
    """é…ç½®åŠ è½½å™¨"""
    
    def __init__(self, config_path: str = "config.json"):
        """
        åˆå§‹åŒ–é…ç½®åŠ è½½å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        self.config_data = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_api_config(self, model_name: Optional[str] = None) -> ApiConfig:
        """
        è·å–APIé…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            
        Returns:
            ApiConfig: APIé…ç½®å¯¹è±¡
        """
        api_section = self.config_data.get("api", {})
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä»modelséƒ¨åˆ†è·å–
        if model_name and model_name in self.config_data.get("models", {}):
            model = self.config_data["models"][model_name]
        else:
            model = api_section.get("model", "gpt-4o")
        
        return ApiConfig(
            base_url=api_section.get("base_url", "https://api.openai.com/v1"),
            api_key=api_section.get("api_key", os.getenv("OPENAI_API_KEY", "")),
            model=model,
            timeout=api_section.get("timeout", 120),
            max_retries=api_section.get("max_retries", 3)
        )
    
    def get_direct_concept_config(
        self, 
        model_name: Optional[str] = None,
        filename_pattern: Optional[str] = None,
        output_name: Optional[str] = None,
        **overrides
    ) -> DirectConceptConfig:
        """
        è·å–DirectConceptConfigé…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            filename_pattern: æ–‡ä»¶åæ¨¡å¼
            output_name: è¾“å‡ºåç§°
            **overrides: è¦†ç›–çš„é…ç½®å‚æ•°
            
        Returns:
            DirectConceptConfig: ç›´æ¥æ¦‚å¿µæå–é…ç½®
        """
        # è·å–æ¨¡å‹è·¯å¾„
        if model_name and model_name in self.config_data.get("models", {}):
            model_path = self.config_data["models"][model_name]
        else:
            model_path = self.config_data.get("api", {}).get("model", "gpt-4o")
        
        # åˆå¹¶å„éƒ¨åˆ†é…ç½®
        extraction_config = self.config_data.get("extraction", {})
        processing_config = self.config_data.get("processing", {})
        paths_config = self.config_data.get("paths", {})
        logging_config = self.config_data.get("logging", {})
        
        # æ„å»ºé…ç½®å‚æ•°
        config_params = {
            # æ¨¡å‹é…ç½®
            "model_path": model_path,
            "max_new_tokens": extraction_config.get("max_new_tokens", 2048),
            "temperature": extraction_config.get("temperature", 0.1),
            "max_workers": extraction_config.get("max_workers", 3),
            
            # æ•°æ®é…ç½®
            "data_directory": paths_config.get("data_directory", "../example_data"),
            "filename_pattern": filename_pattern or paths_config.get("default_filename_pattern", "sample"),
            "output_directory": os.path.join(
                paths_config.get("output_directory", "output"),
                output_name or "default"
            ),
            
            # å¤„ç†é…ç½®
            "batch_size_concept": extraction_config.get("batch_size_concept", 8),
            "text_chunk_size": extraction_config.get("text_chunk_size", 1024),
            "chunk_overlap": extraction_config.get("chunk_overlap", 100),
            
            # æå–é…ç½®
            "extraction_mode": processing_config.get("extraction_mode", "passage_concept"),
            "language": processing_config.get("language", "en"),
            
            # å›¾æ„å»ºé…ç½®
            "include_abstraction_levels": processing_config.get("include_abstraction_levels", True),
            "include_hierarchical_relations": processing_config.get("include_hierarchical_relations", True),
            "min_concept_frequency": processing_config.get("min_concept_frequency", 1),
            
            # è´¨é‡æ§åˆ¶
            "normalize_concept_names": processing_config.get("normalize_concept_names", True),
            "filter_low_quality_concepts": processing_config.get("filter_low_quality_concepts", True),
            "remove_doc_spaces": processing_config.get("remove_doc_spaces", True),
            
            # è°ƒè¯•é…ç½®
            "debug_mode": logging_config.get("debug_mode", True),
            "record_usage": logging_config.get("record_usage", True),
            "save_intermediate_results": logging_config.get("save_intermediate_results", True)
        }
        
        # åº”ç”¨è¦†ç›–å‚æ•°
        config_params.update(overrides)
        
        return DirectConceptConfig(**config_params)
    
    def list_available_models(self) -> Dict[str, str]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        return self.config_data.get("models", {})
    
    def get_model_by_name(self, model_name: str) -> str:
        """æ ¹æ®ç®€çŸ­åç§°è·å–å®Œæ•´æ¨¡å‹å"""
        models = self.config_data.get("models", {})
        if model_name in models:
            return models[model_name]
        else:
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: {list(models.keys())}")
    
    def update_config(self, section: str, key: str, value: Any):
        """æ›´æ–°é…ç½®å€¼"""
        if section not in self.config_data:
            self.config_data[section] = {}
        self.config_data[section][key] = value
    
    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config_data, f, ensure_ascii=False, indent=2)
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("ğŸ“‹ Configuration Summary:")
        print("=" * 50)
        
        # APIé…ç½®
        api_config = self.get_api_config()
        print(f"ğŸ”— API URL: {api_config.base_url}")
        print(f"ğŸ¤– Default Model: {api_config.model}")
        print(f"ğŸ”‘ API Key: {api_config.api_key[:10]}...{api_config.api_key[-4:]}")
        
        # å¯ç”¨æ¨¡å‹
        models = self.list_available_models()
        print(f"\nğŸ¯ Available Models ({len(models)}):")
        for short_name, full_name in models.items():
            print(f"   {short_name}: {full_name}")
        
        # å¤„ç†é…ç½®
        extraction = self.config_data.get("extraction", {})
        processing = self.config_data.get("processing", {})
        print(f"\nâš™ï¸ Processing Settings:")
        print(f"   Batch Size: {extraction.get('batch_size_concept', 8)}")
        print(f"   Chunk Size: {extraction.get('text_chunk_size', 1024)}")
        print(f"   Temperature: {extraction.get('temperature', 0.1)}")
        print(f"   Mode: {processing.get('extraction_mode', 'passage_concept')}")
        print(f"   Language: {processing.get('language', 'en')}")


def create_model_client(config_loader: ConfigLoader, model_name: Optional[str] = None):
    """
    æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
    
    Args:
        config_loader: é…ç½®åŠ è½½å™¨
        model_name: æ¨¡å‹åç§°
        
    Returns:
        LLMGenerator: æ¨¡å‹ç”Ÿæˆå™¨
    """
    import sys
    sys.path.append('..')
    
    api_config = config_loader.get_api_config(model_name)
    
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨OpenAI APIæ ¼å¼
    if "openai" in api_config.base_url.lower() or api_config.base_url.endswith("/v1/openai"):
        from openai import OpenAI
        
        client = OpenAI(
            api_key=api_config.api_key,
            base_url=api_config.base_url,
            timeout=api_config.timeout,
            max_retries=api_config.max_retries
        )
        
        from atlas_rag.llm_generator import LLMGenerator
        return LLMGenerator(client, model_name=api_config.model)
    
    else:
        # æœ¬åœ°æ¨¡å‹æˆ–å…¶ä»–API
        from transformers import pipeline
        from atlas_rag.llm_generator import LLMGenerator
        
        # æœ¬åœ°æ¨¡å‹é…ç½®
        performance_config = config_loader.config_data.get("performance", {})
        
        client = pipeline(
            "text-generation",
            model=api_config.model,
            device_map=performance_config.get("device_map", "auto"),
            load_in_8bit=performance_config.get("load_in_8bit", False),
            load_in_4bit=performance_config.get("load_in_4bit", False)
        )
        
        return LLMGenerator(client, model_name=api_config.model)


# ä¾¿æ·å‡½æ•°
def load_config(config_path: str = "config.json") -> ConfigLoader:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    return ConfigLoader(config_path)


def quick_setup(model_name: Optional[str] = None, config_path: str = "config.json"):
    """
    å¿«é€Ÿè®¾ç½®ï¼šåŠ è½½é…ç½®å¹¶åˆ›å»ºæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        tuple: (config_loader, model, model_name)
    """
    config_loader = load_config(config_path)
    model = create_model_client(config_loader, model_name)
    
    api_config = config_loader.get_api_config(model_name)
    
    return config_loader, model, api_config.model