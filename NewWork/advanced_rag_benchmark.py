#!/usr/bin/env python3
"""
Advanced RAG Benchmark for NewWork Concept Graph
é›†æˆAutoSchemaKGæ‰€æœ‰é«˜çº§RAGæ–¹æ³•çš„å®Œæ•´æµ‹è¯•ç³»ç»Ÿ
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple

# æ·»åŠ è·¯å¾„
sys.path.append('..')
sys.path.append('.')

from config_loader import ConfigLoader, create_model_client
<<<<<<< HEAD
from rag_benchmark import NewWorkToAtlasConverter
=======

import pandas as pd
import pickle
import networkx as nx
from pathlib import Path


class HotpotKGToAtlasConverter:
    """å°†HotpotQAçŸ¥è¯†å›¾è°±è½¬æ¢ä¸ºAtlas RAGå…¼å®¹æ ¼å¼"""
    
    def __init__(self, output_path: str):
        """
        Args:
            output_path: HotpotKGè¾“å‡ºè·¯å¾„ï¼Œå¦‚ "output/hotpot_kg"
        """
        self.output_path = Path(output_path)
        
        # HotpotQAæ•°æ®æ ¼å¼
        concept_csv_dir = self.output_path / "concept_csv"
        self.concepts_csv = concept_csv_dir / "concepts_hotpot_kg.csv"
        self.relationships_csv = concept_csv_dir / "relationships_hotpot_kg.csv"
        
        # å›¾è°±æ–‡ä»¶
        graph_dir = self.output_path / "graph"
        pkl_files = list(graph_dir.glob("*.pkl"))
        if pkl_files:
            self.graph_pkl = pkl_files[0]
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°PKLå›¾è°±æ–‡ä»¶åœ¨: {graph_dir}")
            
    def load_concept_graph(self) -> nx.Graph:
        """åŠ è½½æ¦‚å¿µå›¾"""
        print(f"ğŸ”„ åŠ è½½å›¾è°±æ–‡ä»¶: {self.graph_pkl}")
        with open(self.graph_pkl, 'rb') as f:
            return pickle.load(f)
    
    def load_concepts_and_relations(self) -> tuple:
        """åŠ è½½æ¦‚å¿µå’Œå…³ç³»æ•°æ®"""
        concepts_df = pd.read_csv(self.concepts_csv)
        relationships_df = pd.read_csv(self.relationships_csv)
        return concepts_df, relationships_df
    
    def convert_to_atlas_format(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºAtlas RAGå…¼å®¹æ ¼å¼"""
        print("ğŸ”„ Converting HotpotQA graph to Atlas format...")
        
        # åŠ è½½æ•°æ®
        G = self.load_concept_graph()
        concepts_df, relationships_df = self.load_concepts_and_relations()
        
        # æ„å»ºèŠ‚ç‚¹åˆ—è¡¨ - é€‚é…HotpotQAæ ¼å¼
        node_list = []
        node_dict = {}
        
        for idx, row in concepts_df.iterrows():
            node_id = str(idx)
            node_data = {
                'id': row['id'],
                'text': row['text'] if pd.notna(row['text']) else row['id'],
                'type': row['type'],
                'abstraction_level': row['abstraction_level']
            }
            node_list.append(node_id)
            node_dict[node_id] = node_data
            
            # æ·»åŠ åˆ°NetworkXå›¾ä¸­
            if node_id not in G.nodes:
                G.add_node(node_id, **node_data)
        
        # æ„å»ºè¾¹åˆ—è¡¨ - é€‚é…HotpotQAæ ¼å¼
        edge_list = []
        edge_dict = {}
        
        for idx, row in relationships_df.iterrows():
            source_name = row['source']
            target_name = row['target']
            
            # æŸ¥æ‰¾å¯¹åº”çš„èŠ‚ç‚¹ID
            source_id = None
            target_id = None
            
            for node_idx, concept_row in concepts_df.iterrows():
                if concept_row['id'] == source_name:
                    source_id = str(node_idx)
                if concept_row['id'] == target_name:
                    target_id = str(node_idx)
            
            if source_id and target_id:
                edge_key = (source_id, target_id)
                edge_list.append(edge_key)
                
                edge_data = {
                    'relation': row['relation'],
                    'description': row.get('relation_type', row['relation'])
                }
                edge_dict[edge_key] = edge_data
                
                # æ·»åŠ åˆ°NetworkXå›¾ä¸­
                G.add_edge(source_id, target_id, **edge_data)
        
        print(f"âœ… Converted: {len(node_list)} nodes, {len(edge_list)} edges")
        
        return {
            'KG': G,
            'node_list': node_list,
            'edge_list': edge_list,
            'node_dict': node_dict,
            'edge_dict': edge_dict,
            'original_text_dict': node_dict,
            'original_text_dict_with_node_id': {node_id: data['text'] for node_id, data in node_dict.items()}
        }
>>>>>>> 0ff854f19280eadc04f4289414abf37019510f1e


class CompatibleEmbeddingWrapper:
    """å…¼å®¹AutoSchemaKGçš„embeddingåŒ…è£…å™¨"""
    
    def __init__(self, sentence_encoder):
        self.sentence_encoder = sentence_encoder
        # ç›´æ¥è·å–SentenceTransformerå¯¹è±¡
        self.transformer = sentence_encoder.sentence_encoder
    
    def encode(self, query, query_type=None, **kwargs):
        """
        å…¼å®¹query_typeå‚æ•°çš„encodeæ–¹æ³•
        
        Args:
            query: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            query_type: æŸ¥è¯¢ç±»å‹ ('node', 'edge', 'passage', ç­‰)
            **kwargs: å…¶ä»–å‚æ•°
        """
        try:
            # ç›´æ¥ä½¿ç”¨SentenceTransformerå¯¹è±¡ï¼Œå¿½ç•¥query_type
            return self.transformer.encode(query)
        except Exception as e:
            print(f"âš ï¸ Direct encoding failed: {e}")
            # å°è¯•åŒ…è£…æˆåˆ—è¡¨
            try:
                if isinstance(query, str):
                    return self.transformer.encode([query])[0]
                else:
                    return self.transformer.encode(query)
            except Exception as e2:
                print(f"âŒ All encoding methods failed: {e2}")
                # æœ€åçš„å›é€€ï¼šåˆ›å»ºé›¶å‘é‡
                import numpy as np
                if isinstance(query, list):
                    return np.zeros((len(query), 384))
                else:
                    return np.zeros(384)


class AdvancedRAGTester:
    """é«˜çº§RAGæµ‹è¯•å™¨ï¼Œé›†æˆæ‰€æœ‰AutoSchemaKG RAGæ–¹æ³•"""
    
    def __init__(self, config_loader: ConfigLoader, atlas_data: Dict[str, Any]):
        self.config_loader = config_loader
        self.atlas_data = atlas_data
        self.model = create_model_client(config_loader)
        
        # è®¾ç½®å…¼å®¹çš„sentence encoder
        self._setup_compatible_encoder()
        
        # å‡†å¤‡é«˜çº§RAGæ‰€éœ€çš„æ•°æ®
        self._prepare_advanced_data()
    
    def _setup_compatible_encoder(self):
        """è®¾ç½®å…¼å®¹çš„å¥å­ç¼–ç å™¨"""
        try:
            from sentence_transformers import SentenceTransformer
            from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
            
            # æ­£ç¡®åˆ›å»ºSentenceTransformerå¯¹è±¡
            transformer = SentenceTransformer("all-MiniLM-L6-v2")
            base_encoder = SentenceEmbedding(transformer)
            self.sentence_encoder = CompatibleEmbeddingWrapper(base_encoder)
            print("âœ… Compatible sentence encoder loaded")
        except ImportError as e:
            print(f"âŒ Failed to load sentence encoder: {e}")
            raise
    
    def _prepare_advanced_data(self):
        """å‡†å¤‡é«˜çº§RAGæ–¹æ³•æ‰€éœ€çš„æ•°æ®æ ¼å¼"""
        print("ğŸ”„ Preparing data for advanced RAG methods...")
        
        # 1. åˆ›å»ºembeddings
        self._create_embeddings_if_needed()
        
        # 2. å‡†å¤‡text_dictï¼ˆç”¨äºHippoRAGï¼‰
        text_dict = {}
        for node_id in self.atlas_data['node_list']:
            node_data = self.atlas_data['KG'].nodes[node_id]
            text = f"{node_data.get('id', '')} {node_data.get('text', '')}"
            text_dict[node_id] = text
            
        # 3. å‡†å¤‡text_embeddingsï¼ˆç”¨äºHippoRAG2å’ŒSimpleTextï¼‰
        text_embeddings = {}
        text_embeddings_list = []  # ç”¨äºSimpleTextçš„åˆ—è¡¨æ ¼å¼
        
        for node_id, text in text_dict.items():
            try:
                emb = self.sentence_encoder.transformer.encode(text)
                # ç¡®ä¿æ˜¯1Dæ•°ç»„ï¼Œä½†ä¿æŒ2Dæ ¼å¼ç»™HippoRAG2
                if len(emb.shape) == 1:
                    emb = emb.reshape(1, -1)  # è½¬æ¢ä¸º(1, embedding_dim)
                elif len(emb.shape) > 2:
                    emb = emb.reshape(1, -1)  # å±•å¹³å¹¶è½¬æ¢ä¸º(1, embedding_dim)
                    
                text_embeddings[node_id] = emb
                text_embeddings_list.append(emb.flatten())  # SimpleTextéœ€è¦1D
            except Exception as e:
                print(f"âš ï¸ Failed to encode text for {node_id}: {e}")
                zero_emb_2d = np.zeros((1, 384))  # HippoRAG2éœ€è¦2D
                zero_emb_1d = np.zeros(384)  # SimpleTextéœ€è¦1D
                text_embeddings[node_id] = zero_emb_2d
                text_embeddings_list.append(zero_emb_1d)
        
        print(f"ğŸ” Created text_embeddings for {len(text_embeddings)} nodes")
        
        # 4. å‡†å¤‡file_idå’Œå¿…è¦å±æ€§ï¼ˆç”¨äºHippoRAGç³»åˆ—ï¼‰
        # HippoRAGéœ€è¦æ¯ä¸ªKGèŠ‚ç‚¹éƒ½æœ‰file_idã€type='passage'ã€idå±æ€§
        file_id_dict = {}
        file_id_to_node_id = {}  # HippoRAGéœ€è¦è¿™ä¸ªæ˜ å°„
        
        print(f"ğŸ” Debug: Before modification - KG has {len(self.atlas_data['KG'].nodes)} nodes")
        
        for i, node_id in enumerate(self.atlas_data['node_list']):
            file_id = f"doc_{i}"
            file_id_dict[node_id] = file_id
            
            # ç›´æ¥åœ¨KGèŠ‚ç‚¹ä¸­æ·»åŠ å¿…è¦å±æ€§
            if node_id in self.atlas_data['KG'].nodes:
                # HippoRAGæœŸæœ›file_idæ˜¯å•ä¸ªæ–‡ä»¶ID
                self.atlas_data['KG'].nodes[node_id]['file_id'] = file_id
                # å¼ºåˆ¶è®¾ç½®ä¸ºpassageç±»å‹ï¼ˆHippoRAGåªå¤„ç†passageç±»å‹ï¼‰
                self.atlas_data['KG'].nodes[node_id]['type'] = 'passage'
                # ç¡®ä¿æœ‰idå±æ€§ï¼ˆä¿ç•™åŸæœ‰çš„idæˆ–ä½¿ç”¨node_idï¼‰
                if 'id' not in self.atlas_data['KG'].nodes[node_id]:
                    self.atlas_data['KG'].nodes[node_id]['id'] = node_id
                
                # æ„å»ºfile_id_to_node_idæ˜ å°„ï¼ˆHippoRAGå†…éƒ¨éœ€è¦ï¼‰
                if file_id not in file_id_to_node_id:
                    file_id_to_node_id[file_id] = []
                file_id_to_node_id[file_id].append(node_id)
            else:
                print(f"âš ï¸ Debug: Node {node_id} not found in KG!")
        
        print(f"ğŸ” Debug: After modification - added passage type to {len(file_id_dict)} nodes")
        
        # ä¸ºäº†HippoRAGçš„å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ passageç±»å‹çš„èŠ‚ç‚¹åˆ°text_dict
        for node_id in self.atlas_data['node_list']:
            if node_id not in text_dict and node_id in self.atlas_data['KG'].nodes:
                node_data = self.atlas_data['KG'].nodes[node_id]
                text_dict[node_id] = f"{node_data.get('id', '')} {node_data.get('text', '')}"
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æˆ‘ä»¬è®¾ç½®çš„èŠ‚ç‚¹å±æ€§
        print(f"ğŸ” Debug: Setting up {len(file_id_to_node_id)} file_ids for HippoRAG")
        print(f"ğŸ” Debug: file_id_to_node_id keys: {list(file_id_to_node_id.keys())[:5]}...")
        
        # éªŒè¯ä¸€äº›èŠ‚ç‚¹çš„å±æ€§
        sample_node_ids = list(self.atlas_data['node_list'])[:3]
        for node_id in sample_node_ids:
            if node_id in self.atlas_data['KG'].nodes:
                node_data = self.atlas_data['KG'].nodes[node_id]
                print(f"ğŸ” Debug: Node {node_id} - type: {node_data.get('type')}, file_id: {node_data.get('file_id')}")
            else:
                print(f"âš ï¸ Debug: Node {node_id} not found in KG!")
        
        # 6. ä¸ºHippoRAG2å’ŒSimpleTextå‡†å¤‡æ­£ç¡®æ ¼å¼çš„text_embeddingsæ•°ç»„
        # HippoRAG2æœŸæœ›text_embeddingsæ˜¯2Dæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(num_passages, embedding_dim)
        text_embeddings_array = []
        text_id_list = []  # ä¿æŒé¡ºåºä¸€è‡´
        
        for node_id in self.atlas_data['node_list']:
            if node_id in text_embeddings:
                emb = text_embeddings[node_id]
                # ç¡®ä¿æ˜¯1Dæ ¼å¼
                if len(emb.shape) > 1:
                    emb = emb.flatten()
                text_embeddings_array.append(emb)
                text_id_list.append(node_id)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        text_embeddings_final = np.array(text_embeddings_array) if text_embeddings_array else np.zeros((1, 384))
        
        print(f"ğŸ” Final text_embeddings array shape: {text_embeddings_final.shape}")
        
        # 5. æ›´æ–°atlas_data
        self.atlas_data.update({
            'node_embeddings': self.atlas_data.get('node_embeddings', np.array([])),
            'edge_embeddings': self.atlas_data.get('edge_embeddings', np.array([])),
            'text_dict': text_dict,
            'text_embeddings': text_embeddings_final,  # HippoRAG2å’ŒSimpleTextéœ€è¦çš„æ•°ç»„æ ¼å¼
            'text_embeddings_dict': text_embeddings,  # ä¿ç•™å­—å…¸æ ¼å¼ï¼ˆå¦‚æœå…¶ä»–åœ°æ–¹éœ€è¦ï¼‰
            'text_id_list': text_id_list,  # HippoRAG2éœ€è¦çš„IDåˆ—è¡¨
            'original_text_dict': text_dict.copy(),  # ä¿å­˜åŸå§‹æ–‡æœ¬
            'file_id': file_id_dict,  # æ·»åŠ æ–‡ä»¶IDæ”¯æŒ
            'file_id_to_node_id': file_id_to_node_id  # HippoRAGéœ€è¦çš„æ˜ å°„
        })
        
        print("âœ… Advanced data preparation completed")
    
    def _create_embeddings_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œåˆ›å»ºembeddings"""
        if 'node_embeddings' not in self.atlas_data:
            print("ğŸ”„ Creating embeddings for advanced RAG...")
            
            # å‡†å¤‡èŠ‚ç‚¹æ–‡æœ¬
            node_texts = []
            for node_id in self.atlas_data['node_list']:
                node_data = self.atlas_data['KG'].nodes[node_id]
                text = f"{node_data.get('id', '')} {node_data.get('text', '')}"
                node_texts.append(text)
            
            # å‡†å¤‡è¾¹æ–‡æœ¬
            edge_texts = []
            for edge in self.atlas_data['edge_list']:
                source_node = self.atlas_data['KG'].nodes[edge[0]]
                target_node = self.atlas_data['KG'].nodes[edge[1]]
                edge_data = self.atlas_data['KG'].edges[edge]
                
                text = f"{source_node.get('id', '')} {edge_data.get('relation', '')} {target_node.get('id', '')}"
                edge_texts.append(text)
            
            # ä½¿ç”¨batchæ–¹å¼è®¡ç®—embeddingsï¼ˆæŒ‰ç…§atlas_ragçš„æ ‡å‡†æ–¹å¼ï¼‰
            node_embeddings = self._compute_embeddings_in_batches(node_texts, batch_size=16)
            edge_embeddings = self._compute_embeddings_in_batches(edge_texts, batch_size=16)
            
            # åˆ›å»ºFAISSç´¢å¼•
            import faiss
            import numpy as np
            
            if len(node_embeddings) > 0:
                node_embeddings_array = np.array(node_embeddings)
                print(f"ğŸ” Node embeddings shape: {node_embeddings_array.shape}")
                node_faiss_index = faiss.IndexFlatIP(node_embeddings_array.shape[1])
                node_faiss_index.add(node_embeddings_array.astype('float32'))
            else:
                print("âš ï¸ No node embeddings created!")
                node_faiss_index = None
                node_embeddings_array = np.zeros((1, 384))  # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„embedding
            
            if len(edge_embeddings) > 0:
                edge_embeddings_array = np.array(edge_embeddings)
                print(f"ğŸ” Edge embeddings shape: {edge_embeddings_array.shape}")
                edge_faiss_index = faiss.IndexFlatIP(edge_embeddings_array.shape[1])
                edge_faiss_index.add(edge_embeddings_array.astype('float32'))
            else:
                print("âš ï¸ No edge embeddings created!")
                edge_faiss_index = None
                edge_embeddings_array = np.zeros((1, 384))  # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„embedding
            
            self.atlas_data.update({
                'node_embeddings': node_embeddings_array,
                'edge_embeddings': edge_embeddings_array,
                'node_faiss_index': node_faiss_index,
                'edge_faiss_index': edge_faiss_index
            })
    
    def _compute_embeddings_in_batches(self, texts: List[str], batch_size: int = 16) -> List:
        """æŒ‰æ‰¹æ¬¡è®¡ç®—embeddings"""
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                # ç›´æ¥ä½¿ç”¨SentenceTransformer
                batch_embeddings = self.sentence_encoder.transformer.encode(batch)
                
                # å¦‚æœè¿”å›çš„æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                if hasattr(batch_embeddings, 'tolist'):
                    batch_embeddings = batch_embeddings.tolist()
                
                all_embeddings.extend(batch_embeddings)
                
            except Exception as e:
                print(f"âš ï¸ Batch embedding failed, trying individual encoding: {e}")
                # å¦‚æœæ‰¹æ¬¡å¤±è´¥ï¼Œé€ä¸ªç¼–ç 
                for text in batch:
                    try:
                        embedding = self.sentence_encoder.transformer.encode([text])
                        if hasattr(embedding, 'tolist'):
                            if len(embedding) > 0:
                                all_embeddings.append(embedding[0].tolist() if hasattr(embedding[0], 'tolist') else embedding[0])
                            else:
                                all_embeddings.append([0.0] * 384)  # 384ç»´å ä½ç¬¦
                        else:
                            all_embeddings.append(embedding[0] if len(embedding) > 0 else [0.0] * 384)
                    except Exception as text_e:
                        print(f"âŒ Failed to encode text: {text[:50]}... Error: {text_e}")
                        # æ·»åŠ é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                        all_embeddings.append([0.0] * 384)
        
        return all_embeddings
    
    def test_simple_graph_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•ç®€å•å›¾æ£€ç´¢å™¨ï¼ˆåŸç‰ˆï¼‰"""
        try:
            from atlas_rag.retriever.simple_retriever import SimpleGraphRetriever
            
            print(f"ğŸ” SimpleGraphRetriever results for '{query}':")
            
            retriever = SimpleGraphRetriever(
                llm_generator=self.model,
                sentence_encoder=self.sentence_encoder,
                data=self.atlas_data
            )
            
            results, scores = retriever.retrieve(query, topN=topN)
            
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result}")
            
            return results, "SimpleGraphRetriever"
            
        except Exception as e:
            print(f"âŒ SimpleGraphRetriever failed: {e}")
            return [], "SimpleGraphRetriever (failed)"
    
    def test_tog_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•ToGæ£€ç´¢å™¨"""
        try:
            from atlas_rag.retriever.tog import TogRetriever
            from atlas_rag.retriever.inference_config import InferenceConfig
            
            print(f"ğŸ” ToGRetriever results for '{query}':")
            
            # åˆ›å»ºæ¨ç†é…ç½®
            inference_config = InferenceConfig()
            
            retriever = TogRetriever(
                llm_generator=self.model,
                sentence_encoder=self.sentence_encoder,
                data=self.atlas_data,
                inference_config=inference_config
            )
            
            # ToGçš„retrieveæ–¹æ³•è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
            result = retriever.retrieve(query, topN=topN)
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if isinstance(result, tuple) and len(result) == 2:
                paths, explanations = result
                for path, explanation in zip(paths, explanations):
                    # ç§»é™¤æ‹¬å·å’Œå¼•å·
                    if isinstance(path, str):
                        path = path.strip('()').replace("'", "")
                        # å¦‚æœè§£é‡Šä¸æ˜¯N/Aï¼Œæ·»åŠ è§£é‡Š
                        if explanation != "N/A":
                            formatted_results.append(f"{path} ({explanation})")
                        else:
                            formatted_results.append(path)
            else:
                formatted_results = [str(result)]
            
            print("   Retrieved paths:")
            for i, result in enumerate(formatted_results, 1):
                print(f"   {i}. {result}")
            
            return formatted_results, "ToGRetriever"
            
        except Exception as e:
            print(f"âŒ ToGRetriever failed: {e}")
            import traceback
            traceback.print_exc()
            return [], "ToGRetriever (failed)"
    
    def test_hipporag_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•HippoRAGæ£€ç´¢å™¨"""
        try:
            from atlas_rag.retriever.hipporag import HippoRAGRetriever
            
            print(f"ğŸ” HippoRAGRetriever results for '{query}':")
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” Debug: KG has {len(self.atlas_data['KG'].nodes)} nodes")
            passage_nodes = [n for n in self.atlas_data['KG'].nodes if self.atlas_data['KG'].nodes[n].get('type') == 'passage']
            print(f"ğŸ” Debug: Found {len(passage_nodes)} passage nodes")
            
            retriever = HippoRAGRetriever(
                llm_generator=self.model,
                sentence_encoder=self.sentence_encoder,
                data=self.atlas_data
            )
            
            # è°ƒè¯•HippoRAGå†…éƒ¨çš„file_id_to_node_idæ˜ å°„
            print(f"ğŸ” Debug: HippoRAG file_id_to_node_id has {len(retriever.file_id_to_node_id)} entries")
            if retriever.file_id_to_node_id:
                sample_keys = list(retriever.file_id_to_node_id.keys())[:3]
                print(f"ğŸ” Debug: Sample file_id keys: {sample_keys}")
            
            results, scores = retriever.retrieve(query, topN=topN)
            
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result}")
            
            return results, "HippoRAGRetriever"
            
        except Exception as e:
            print(f"âŒ HippoRAGRetriever failed: {e}")
            import traceback
            traceback.print_exc()
            return [], "HippoRAGRetriever (failed)"
    
    def test_hipporag2_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•HippoRAG2æ£€ç´¢å™¨"""
        try:
            from atlas_rag.retriever.hipporag2 import HippoRAG2Retriever
            
            print(f"ğŸ” HippoRAG2Retriever results for '{query}':")
            
            # è°ƒè¯•text_embeddingsæ ¼å¼
            if 'text_embeddings' in self.atlas_data:
                print(f"ğŸ” Debug: text_embeddings array shape: {self.atlas_data['text_embeddings'].shape}")
                if 'text_id_list' in self.atlas_data:
                    print(f"ğŸ” Debug: text_id_list has {len(self.atlas_data['text_id_list'])} entries")
                else:
                    print("âš ï¸ Debug: text_id_list missing!")
            
            retriever = HippoRAG2Retriever(
                llm_generator=self.model,
                sentence_encoder=self.sentence_encoder,
                data=self.atlas_data
            )
            
            results, scores = retriever.retrieve(query, topN=topN)
            
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result}")
            
            return results, "HippoRAG2Retriever"
            
        except Exception as e:
            print(f"âŒ HippoRAG2Retriever failed: {e}")
            import traceback
            traceback.print_exc()
            return [], "HippoRAG2Retriever (failed)"
    
    def test_simple_text_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•ç®€å•æ–‡æœ¬æ£€ç´¢å™¨"""
        try:
            from atlas_rag.retriever.simple_retriever import SimpleTextRetriever
            
            print(f"ğŸ” SimpleTextRetriever results for '{query}':")
            
            # æ„å»ºpassageå­—å…¸
            passage_dict = {}
            for node_id in self.atlas_data['node_list']:
                node_data = self.atlas_data['KG'].nodes[node_id]
                passage_dict[node_id] = node_data.get('text', node_data.get('id', ''))
            
            # text_embeddingsç°åœ¨å·²ç»æ˜¯æ­£ç¡®çš„æ•°ç»„æ ¼å¼
            corrected_data = self.atlas_data
            
            retriever = SimpleTextRetriever(
                passage_dict=passage_dict,
                sentence_encoder=self.sentence_encoder,
                data=corrected_data
            )
            
            results, scores = retriever.retrieve(query, topN=topN)
            
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result}")
            
            return results, "SimpleTextRetriever"
            
        except Exception as e:
            print(f"âŒ SimpleTextRetriever failed: {e}")
            return [], "SimpleTextRetriever (failed)"
    
    def test_raptor_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•RAPTORæ£€ç´¢å™¨ - å±‚æ¬¡åŒ–èšç±»æ£€ç´¢"""
        try:
            print(f"ğŸ” RAPTORRetriever results for '{query}':")
            
            # RAPTORåŸºç¡€å®ç°ï¼šåŸºäºå±‚æ¬¡åŒ–èšç±»çš„æ£€ç´¢
            # 1. è·å–æŸ¥è¯¢embedding
            query_embedding = self.sentence_encoder.encode([query])
            if len(query_embedding.shape) > 1:
                query_embedding = query_embedding.flatten()
            
            # 2. è®¡ç®—ä¸æ‰€æœ‰èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
            similarities = []
            valid_nodes = []
            
            for node_id in self.atlas_data['node_list']:
                if 'text_embeddings' in self.atlas_data:
                    # ä½¿ç”¨é¢„è®¡ç®—çš„embeddings
                    try:
                        idx = list(self.atlas_data['node_list']).index(node_id)
                        node_emb = self.atlas_data['text_embeddings'][idx]
                        if len(node_emb.shape) > 1:
                            node_emb = node_emb.flatten()
                        
                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        sim = np.dot(query_embedding, node_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_emb))
                        similarities.append(sim)
                        valid_nodes.append(node_id)
                    except:
                        continue
            
            # 3. RAPTORç‰¹è‰²ï¼šå±‚æ¬¡åŒ–èšç±» - ç®€åŒ–ç‰ˆæœ¬
            # é€‰æ‹©topèŠ‚ç‚¹ï¼Œç„¶ååŸºäºå›¾ç»“æ„æ‰©å±•é‚»å±…èŠ‚ç‚¹
            if similarities:
                # è·å–æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹
                top_indices = np.argsort(similarities)[-topN*2:][::-1]  # è·å–2å€æ•°é‡ç”¨äºèšç±»
                top_nodes = [valid_nodes[i] for i in top_indices[:min(len(top_indices), len(valid_nodes))]]
                
                # å±‚æ¬¡åŒ–æ‰©å±•ï¼šæ·»åŠ é‚»å±…èŠ‚ç‚¹
                expanded_nodes = set(top_nodes)
                for node in top_nodes[:topN//2 + 1]:  # åªå¯¹topå‡ ä¸ªèŠ‚ç‚¹æ‰©å±•
                    if node in self.atlas_data['KG'].nodes:
                        neighbors = list(self.atlas_data['KG'].neighbors(node))
                        expanded_nodes.update(neighbors[:2])  # æ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæ·»åŠ 2ä¸ªé‚»å±…
                
                # è·å–æœ€ç»ˆç»“æœ
                results = []
                for node_id in list(expanded_nodes)[:topN]:
                    if node_id in self.atlas_data['KG'].nodes:
                        node_data = self.atlas_data['KG'].nodes[node_id]
                        text = node_data.get('text', node_data.get('id', str(node_id)))
                        results.append(f"RAPTOR: {text}")
                
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result}")
                
                return results, "RAPTORRetriever"
            else:
                print("   No valid embeddings found")
                return [], "RAPTORRetriever"
                
        except Exception as e:
            print(f"âŒ RAPTORRetriever failed: {e}")
            import traceback
            traceback.print_exc()
            return [], "RAPTORRetriever (failed)"
    
    def test_graphrag_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•GraphRAGæ£€ç´¢å™¨ - å¾®è½¯GraphRAGé£æ ¼çš„å…¨å±€+æœ¬åœ°æ£€ç´¢"""
        try:
            print(f"ğŸ” GraphRAGRetriever results for '{query}':")
            
            # GraphRAGé£æ ¼ï¼šç»“åˆå…¨å±€æ€»ç»“å’Œæœ¬åœ°æ£€ç´¢
            # 1. å…¨å±€æ£€ç´¢ï¼šåŸºäºå›¾çš„æ•´ä½“ç»“æ„
            query_embedding = self.sentence_encoder.encode([query])
            if len(query_embedding.shape) > 1:
                query_embedding = query_embedding.flatten()
            
            # 2. è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ï¼ˆåº¦ä¸­å¿ƒæ€§å’Œembeddingç›¸ä¼¼åº¦çš„ç»„åˆï¼‰
            node_scores = {}
            
            for node_id in self.atlas_data['node_list']:
                try:
                    # è·å–èŠ‚ç‚¹åº¦æ•°ï¼ˆå…¨å±€é‡è¦æ€§ï¼‰
                    degree = self.atlas_data['KG'].degree(node_id) if node_id in self.atlas_data['KG'].nodes else 0
                    degree_score = degree / max(1, max([self.atlas_data['KG'].degree(n) for n in self.atlas_data['KG'].nodes]))
                    
                    # è·å–è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæœ¬åœ°ç›¸å…³æ€§ï¼‰
                    semantic_score = 0
                    if 'text_embeddings' in self.atlas_data:
                        try:
                            idx = list(self.atlas_data['node_list']).index(node_id)
                            node_emb = self.atlas_data['text_embeddings'][idx]
                            if len(node_emb.shape) > 1:
                                node_emb = node_emb.flatten()
                            semantic_score = np.dot(query_embedding, node_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_emb))
                        except:
                            semantic_score = 0
                    
                    # GraphRAGç‰¹è‰²ï¼šå…¨å±€å’Œæœ¬åœ°åˆ†æ•°çš„åŠ æƒç»„åˆ
                    combined_score = 0.3 * degree_score + 0.7 * semantic_score
                    node_scores[node_id] = combined_score
                    
                except Exception as e:
                    continue
            
            # 3. è·å–topèŠ‚ç‚¹å¹¶æ„å»ºç¤¾åŒº
            if node_scores:
                sorted_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)
                top_nodes = [node_id for node_id, score in sorted_nodes[:topN]]
                
                results = []
                for node_id in top_nodes:
                    if node_id in self.atlas_data['KG'].nodes:
                        node_data = self.atlas_data['KG'].nodes[node_id]
                        text = node_data.get('text', node_data.get('id', str(node_id)))
                        score = node_scores[node_id]
                        results.append(f"GraphRAG (score: {score:.3f}): {text}")
                
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result}")
                
                return results, "GraphRAGRetriever"
            else:
                print("   No valid scores computed")
                return [], "GraphRAGRetriever"
                
        except Exception as e:
            print(f"âŒ GraphRAGRetriever failed: {e}")
            import traceback
            traceback.print_exc()
            return [], "GraphRAGRetriever (failed)"
    
    def test_lightrag_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•LightRAGæ£€ç´¢å™¨ - è½»é‡çº§å›¾æ£€ç´¢"""
        try:
            print(f"ğŸ” LightRAGRetriever results for '{query}':")
            
            # LightRAGï¼šè½»é‡çº§çš„å›¾æ£€ç´¢ï¼Œé‡ç‚¹åœ¨æ•ˆç‡
            query_embedding = self.sentence_encoder.encode([query])
            if len(query_embedding.shape) > 1:
                query_embedding = query_embedding.flatten()
            
            # 1. å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            similarities = []
            valid_nodes = []
            
            for node_id in self.atlas_data['node_list'][:20]:  # LightRAGç‰¹è‰²ï¼šé™åˆ¶æœç´¢èŒƒå›´æé«˜æ•ˆç‡
                if 'text_embeddings' in self.atlas_data:
                    try:
                        idx = list(self.atlas_data['node_list']).index(node_id)
                        node_emb = self.atlas_data['text_embeddings'][idx]
                        if len(node_emb.shape) > 1:
                            node_emb = node_emb.flatten()
                        
                        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
                        sim = np.dot(query_embedding, node_emb)  # ä¸è¿›è¡Œå½’ä¸€åŒ–ï¼Œæé«˜é€Ÿåº¦
                        similarities.append(sim)
                        valid_nodes.append(node_id)
                    except:
                        continue
            
            # 2. è½»é‡çº§æ‰©å±•ï¼šåªåŸºäºç›´æ¥é‚»å±…
            if similarities:
                top_indices = np.argsort(similarities)[-topN:][::-1]
                results = []
                
                for idx in top_indices:
                    if idx < len(valid_nodes):
                        node_id = valid_nodes[idx]
                        if node_id in self.atlas_data['KG'].nodes:
                            node_data = self.atlas_data['KG'].nodes[node_id]
                            text = node_data.get('text', node_data.get('id', str(node_id)))
                            results.append(f"LightRAG: {text}")
                
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result}")
                
                return results, "LightRAGRetriever"
            else:
                print("   No valid similarities computed")
                return [], "LightRAGRetriever"
                
        except Exception as e:
            print(f"âŒ LightRAGRetriever failed: {e}")
            return [], "LightRAGRetriever (failed)"
    
    def test_minirag_retriever(self, query: str, topN: int = 3) -> Tuple[List[str], str]:
        """æµ‹è¯•MiniRAGæ£€ç´¢å™¨ - æœ€ç®€åŒ–çš„RAGæ£€ç´¢"""
        try:
            print(f"ğŸ” MiniRAGRetriever results for '{query}':")
            
            # MiniRAGï¼šæœ€ç®€åŒ–çš„å®ç°ï¼Œç›´æ¥åŸºäºæ–‡æœ¬åŒ¹é…
            query_words = set(query.lower().split())
            
            # ç®€å•çš„è¯æ±‡åŒ¹é…å¾—åˆ†
            node_scores = {}
            
            for node_id in self.atlas_data['node_list']:
                if node_id in self.atlas_data['KG'].nodes:
                    node_data = self.atlas_data['KG'].nodes[node_id]
                    text = node_data.get('text', node_data.get('id', '')).lower()
                    
                    # è®¡ç®—è¯æ±‡é‡å å¾—åˆ†
                    text_words = set(text.split())
                    overlap = len(query_words.intersection(text_words))
                    
                    # MiniRAGç‰¹è‰²ï¼šåŠ ä¸Šç®€å•çš„é•¿åº¦æƒ©ç½š
                    length_penalty = 1.0 / (1.0 + len(text_words) / 10.0)
                    score = overlap * length_penalty
                    
                    if score > 0:
                        node_scores[node_id] = score
            
            # è·å–topç»“æœ
            if node_scores:
                sorted_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)
                top_nodes = sorted_nodes[:topN]
                
                results = []
                for node_id, score in top_nodes:
                    if node_id in self.atlas_data['KG'].nodes:
                        node_data = self.atlas_data['KG'].nodes[node_id]
                        text = node_data.get('text', node_data.get('id', str(node_id)))
                        results.append(f"MiniRAG (score: {score:.2f}): {text}")
                
                for i, result in enumerate(results, 1):
                    print(f"   {i}. {result}")
                
                return results, "MiniRAGRetriever"
            else:
                # å¦‚æœæ²¡æœ‰è¯æ±‡åŒ¹é…ï¼Œéšæœºé€‰æ‹©ä¸€äº›èŠ‚ç‚¹
                fallback_results = []
                for node_id in list(self.atlas_data['node_list'])[:topN]:
                    if node_id in self.atlas_data['KG'].nodes:
                        node_data = self.atlas_data['KG'].nodes[node_id]
                        text = node_data.get('text', node_data.get('id', str(node_id)))
                        fallback_results.append(f"MiniRAG (fallback): {text}")
                
                for i, result in enumerate(fallback_results, 1):
                    print(f"   {i}. {result}")
                
                return fallback_results, "MiniRAGRetriever"
                
        except Exception as e:
            print(f"âŒ MiniRAGRetriever failed: {e}")
            return [], "MiniRAGRetriever (failed)"
    
    def run_comprehensive_benchmark(self, test_queries: List[str]) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„RAG benchmark"""
        print("\nğŸš€ Running Comprehensive RAG Benchmark")
        print("=" * 60)
        
        # å®šä¹‰æ‰€æœ‰è¦æµ‹è¯•çš„RAGæ–¹æ³•
        rag_methods = [
            ("simple_graph", self.test_simple_graph_retriever),
            ("tog", self.test_tog_retriever),
            ("hipporag", self.test_hipporag_retriever),
            ("hipporag2", self.test_hipporag2_retriever),
            ("simple_text", self.test_simple_text_retriever),
            ("raptor", self.test_raptor_retriever),
            ("graphrag", self.test_graphrag_retriever),
            ("lightrag", self.test_lightrag_retriever),
            ("minirag", self.test_minirag_retriever),
        ]
        
        results = {
            'queries': test_queries,
            'methods': {},
            'summary': {
                'total_methods': len(rag_methods),
                'successful_methods': 0,
                'failed_methods': 0,
                'method_success_rate': {}
            }
        }
        
        # ä¸ºæ¯ç§æ–¹æ³•åˆå§‹åŒ–ç»“æœå­—å…¸
        for method_name, _ in rag_methods:
            results['methods'][method_name] = {}
            results['summary']['method_success_rate'][method_name] = 0
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢æµ‹è¯•æ‰€æœ‰æ–¹æ³•
        for query_idx, query in enumerate(test_queries, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“ Query {query_idx}/{len(test_queries)}: {query}")
            print(f"{'='*60}")
            
            for method_name, method_func in rag_methods:
                print(f"\nğŸ”¬ Testing {method_name.upper()}...")
                print("-" * 40)
                
                try:
                    retrieved_results, method_status = method_func(query)
                    
                    # è®°å½•ç»“æœ
                    results['methods'][method_name][query] = {
                        'results': retrieved_results,
                        'status': method_status,
                        'success': len(retrieved_results) > 0
                    }
                    
                    if len(retrieved_results) > 0:
                        results['summary']['method_success_rate'][method_name] += 1
                    
                except Exception as e:
                    print(f"âŒ {method_name} failed with error: {e}")
                    results['methods'][method_name][query] = {
                        'results': [],
                        'status': f"{method_name} (error)",
                        'success': False,
                        'error': str(e)
                    }
        
        # è®¡ç®—æˆåŠŸç‡
        for method_name in results['summary']['method_success_rate']:
            success_count = results['summary']['method_success_rate'][method_name]
            results['summary']['method_success_rate'][method_name] = success_count / len(test_queries)
            
            if success_count > 0:
                results['summary']['successful_methods'] += 1
            else:
                results['summary']['failed_methods'] += 1
        
        return results
    
    def print_benchmark_summary(self, results: Dict[str, Any]):
        """æ‰“å°benchmarkæ‘˜è¦"""
        print(f"\nğŸ“Š Comprehensive RAG Benchmark Summary")
        print("=" * 60)
        
        summary = results['summary']
        
        print(f"ğŸ“ˆ Overall Statistics:")
        print(f"   Total RAG Methods Tested: {summary['total_methods']}")
        print(f"   Successful Methods: {summary['successful_methods']}")
        print(f"   Failed Methods: {summary['failed_methods']}")
        print(f"   Total Queries: {len(results['queries'])}")
        
        print(f"\nğŸ¯ Method Success Rates:")
        for method_name, success_rate in summary['method_success_rate'].items():
            status = "âœ…" if success_rate > 0 else "âŒ"
            print(f"   {status} {method_name.upper()}: {success_rate:.1%}")
        
        print(f"\nğŸ” Query Results Preview:")
        for i, query in enumerate(results['queries'][:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªæŸ¥è¯¢çš„ç»“æœ
            print(f"   Query {i}: {query}")
            for method_name in results['methods']:
                if query in results['methods'][method_name]:
                    success = results['methods'][method_name][query]['success']
                    status = "âœ…" if success else "âŒ"
                    print(f"      {status} {method_name}")
    
    def save_comprehensive_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜å…¨é¢çš„æµ‹è¯•ç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Comprehensive results saved to: {output_file}")


def create_advanced_test_queries():
<<<<<<< HEAD
    """åˆ›å»ºé«˜çº§æµ‹è¯•æŸ¥è¯¢"""
    return [
        "Who is Agent Alex Mercer and what is his role?",
        "What is Operation: Dulce and why is it important?",
        "Describe the Paranormal Military Squad",
        "What protocols and procedures are mentioned?",
        "Who shows compliance and what does it mean?",
        "What are the relationships between team members?",
        "What anomalies are being investigated?",
        "Explain the briefing room and its significance"
=======
    """åˆ›å»ºé«˜çº§æµ‹è¯•æŸ¥è¯¢ - é€‚é…HotpotQAæ•°æ®"""
    return [
        "What is the main topic or subject?",
        "Who are the key people mentioned?",
        "What are the important events described?",
        "What locations or places are mentioned?", 
        "What organizations are involved?",
        "What are the main relationships between entities?",
        "What time periods are referenced?",
        "What are the key concepts or ideas discussed?"
>>>>>>> 0ff854f19280eadc04f4289414abf37019510f1e
    ]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ Advanced RAG Benchmark for NewWork Concept Graph")
    print("=" * 70)
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ Loading configuration...")
        config_loader = ConfigLoader()
        
        # 2. è½¬æ¢æ¦‚å¿µå›¾è°±
        print("\nğŸ”„ Converting concept graph...")
<<<<<<< HEAD
        converter = NewWorkToAtlasConverter("output/simple_test")
=======
        converter = HotpotKGToAtlasConverter("output/hotpot_kg")
>>>>>>> 0ff854f19280eadc04f4289414abf37019510f1e
        atlas_data = converter.convert_to_atlas_format()
        
        # 3. åˆå§‹åŒ–é«˜çº§RAGæµ‹è¯•å™¨
        print("\nğŸ¤– Initializing advanced RAG tester...")
        rag_tester = AdvancedRAGTester(config_loader, atlas_data)
        
        # 4. åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
        test_queries = create_advanced_test_queries()
        print(f"\nğŸ“ Created {len(test_queries)} advanced test queries")
        
        # 5. è¿è¡Œå…¨é¢benchmark
        results = rag_tester.run_comprehensive_benchmark(test_queries)
        
        # 6. æ‰“å°æ‘˜è¦
        rag_tester.print_benchmark_summary(results)
        
        # 7. ä¿å­˜ç»“æœ
<<<<<<< HEAD
        output_file = "output/simple_test/advanced_rag_benchmark_results.json"
=======
        output_file = "output/hotpot_kg/advanced_rag_benchmark_results.json"
>>>>>>> 0ff854f19280eadc04f4289414abf37019510f1e
        rag_tester.save_comprehensive_results(results, output_file)
        
        print(f"\nğŸ‰ Advanced RAG Benchmark completed!")
        print(f"ğŸ“Š Results saved to: {output_file}")
        
        # 8. æ˜¾ç¤ºå¯ç”¨çš„RAGæ–¹æ³•
        print(f"\nğŸ¤– AutoSchemaKG RAG Methods Tested:")
        print(f"   1. SimpleGraphRetriever - ç®€å•å›¾æ£€ç´¢")
        print(f"   2. ToGRetriever - Tree of Generationæ£€ç´¢")
        print(f"   3. HippoRAGRetriever - HippoRAGæ£€ç´¢")
        print(f"   4. HippoRAG2Retriever - HippoRAG2æ£€ç´¢")
        print(f"   5. SimpleTextRetriever - ç®€å•æ–‡æœ¬æ£€ç´¢")
        print(f"   6. RAPTORRetriever - å±‚æ¬¡åŒ–èšç±»æ£€ç´¢")
        print(f"   7. GraphRAGRetriever - å¾®è½¯GraphRAGé£æ ¼æ£€ç´¢")
        print(f"   8. LightRAGRetriever - è½»é‡çº§å›¾æ£€ç´¢")
        print(f"   9. MiniRAGRetriever - æœ€ç®€åŒ–RAGæ£€ç´¢")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Advanced benchmark interrupted by user")
    except Exception as e:
        print(f"\nâŒ Advanced benchmark failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()