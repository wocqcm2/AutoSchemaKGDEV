#!/usr/bin/env python3
"""
Standard AutoSchemaKG Benchmark Integration for NewWork
å°†NewWorkæ¦‚å¿µå›¾è°±é›†æˆåˆ°AutoSchemaKGæ ‡å‡†benchmarkç³»ç»Ÿä¸­è¿›è¡Œç§‘å­¦è¯„ä¼°
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import logging
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.append('..')
sys.path.append('.')

from config_loader import ConfigLoader, create_model_client
from rag_benchmark import NewWorkToAtlasConverter

# å¯¼å…¥AutoSchemaKGæ ‡å‡†ç»„ä»¶
from atlas_rag.evaluation.benchmark import RAGBenchmark, BenchMarkConfig
from atlas_rag.evaluation.evaluation import QAJudger
from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
from atlas_rag.retriever.simple_retriever import SimpleGraphRetriever, SimpleTextRetriever
from atlas_rag.retriever.hipporag import HippoRAGRetriever
from atlas_rag.retriever.hipporag2 import HippoRAG2Retriever
from atlas_rag.retriever.tog import TogRetriever

from sentence_transformers import SentenceTransformer
import faiss


class NewWorkStandardBenchmark:
    """ä½¿ç”¨AutoSchemaKGæ ‡å‡†benchmarkè¯„ä¼°NewWork KG"""
    
    def __init__(self, newwork_output_path: str = "output/simple_test"):
        self.newwork_output_path = newwork_output_path
        self.config_loader = ConfigLoader()
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('NewWorkBenchmark')
        logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ handler
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def prepare_newwork_data(self) -> Dict[str, Any]:
        """å‡†å¤‡NewWorkæ•°æ®å¹¶è½¬æ¢ä¸ºAtlasæ ¼å¼"""
        print("ğŸ“Š å‡†å¤‡NewWorkæ¦‚å¿µå›¾è°±æ•°æ®...")
        
        # 1. è½¬æ¢NewWorkæ•°æ®ä¸ºAtlasæ ¼å¼
        converter = NewWorkToAtlasConverter(self.newwork_output_path)
        atlas_data = converter.convert_to_atlas_format()
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {len(atlas_data.get('node_list', []))} èŠ‚ç‚¹, {len(atlas_data.get('edge_list', []))} è¾¹")
        
        # 2. è®¾ç½®sentence encoder
        transformer = SentenceTransformer("all-MiniLM-L6-v2")
        sentence_encoder = SentenceEmbedding(transformer)
        
        # 3. åˆ›å»ºembeddingså’Œç´¢å¼•
        atlas_data = self._create_embeddings_and_indices(atlas_data, sentence_encoder)
        
        return atlas_data, sentence_encoder
    
    def _create_embeddings_and_indices(self, atlas_data: Dict[str, Any], sentence_encoder) -> Dict[str, Any]:
        """åˆ›å»ºembeddingså’ŒFAISSç´¢å¼•"""
        print("ğŸ”„ åˆ›å»ºembeddingså’Œç´¢å¼•...")
        
        try:
            # 1. åˆ›å»ºnode embeddings
            node_texts = []
            for node_id in atlas_data['node_list']:
                if node_id in atlas_data['KG'].nodes:
                    node_data = atlas_data['KG'].nodes[node_id]
                    text = node_data.get('text', node_data.get('id', str(node_id)))
                    node_texts.append(text)
                else:
                    node_texts.append(str(node_id))
            
            if node_texts:
                node_embeddings = sentence_encoder.encode(node_texts)
                if len(node_embeddings.shape) == 1:
                    node_embeddings = node_embeddings.reshape(1, -1)
            else:
                node_embeddings = np.zeros((1, 384))
            
            # 2. åˆ›å»ºedge embeddings
            edge_texts = []
            for edge in atlas_data['edge_list']:
                if len(edge) >= 2:
                    source_data = atlas_data['KG'].nodes.get(edge[0], {})
                    target_data = atlas_data['KG'].nodes.get(edge[1], {})
                    edge_data = atlas_data['KG'].edges.get(edge, {})
                    
                    source_text = source_data.get('text', source_data.get('id', str(edge[0])))
                    target_text = target_data.get('text', target_data.get('id', str(edge[1])))
                    relation = edge_data.get('relation', 'related_to')
                    
                    edge_text = f"{source_text} {relation} {target_text}"
                    edge_texts.append(edge_text)
                else:
                    edge_texts.append("empty_edge")
            
            if edge_texts:
                edge_embeddings = sentence_encoder.encode(edge_texts)
                if len(edge_embeddings.shape) == 1:
                    edge_embeddings = edge_embeddings.reshape(1, -1)
            else:
                edge_embeddings = np.zeros((1, 384))
            
            # 3. åˆ›å»ºFAISSç´¢å¼•
            # Nodeç´¢å¼•
            if node_embeddings.shape[0] > 0 and node_embeddings.shape[1] > 0:
                node_index = faiss.IndexFlatIP(node_embeddings.shape[1])
                # æ ‡å‡†åŒ–embeddings
                normalized_node_emb = node_embeddings / np.linalg.norm(node_embeddings, axis=1, keepdims=True)
                node_index.add(normalized_node_emb.astype('float32'))
            else:
                node_index = faiss.IndexFlatIP(384)
            
            # Edgeç´¢å¼•
            if edge_embeddings.shape[0] > 0 and edge_embeddings.shape[1] > 0:
                edge_index = faiss.IndexFlatIP(edge_embeddings.shape[1])
                # æ ‡å‡†åŒ–embeddings
                normalized_edge_emb = edge_embeddings / np.linalg.norm(edge_embeddings, axis=1, keepdims=True)
                edge_index.add(normalized_edge_emb.astype('float32'))
            else:
                edge_index = faiss.IndexFlatIP(384)
            
            # 4. åˆ›å»ºtext_dictå’Œå…¶ä»–å¿…éœ€æ•°æ®
            text_dict = {}
            for node_id in atlas_data['node_list']:
                if node_id in atlas_data['KG'].nodes:
                    node_data = atlas_data['KG'].nodes[node_id]
                    text_dict[node_id] = node_data.get('text', node_data.get('id', str(node_id)))
                else:
                    text_dict[node_id] = str(node_id)
            
            # 5. æ›´æ–°atlas_data
            atlas_data.update({
                'node_embeddings': node_embeddings,
                'edge_embeddings': edge_embeddings,
                'node_faiss_index': node_index,
                'edge_faiss_index': edge_index,
                'text_dict': text_dict,
                'text_embeddings': node_embeddings,  # ç”¨äºæŸäº›retriever
            })
            
            print(f"âœ… Embeddingsåˆ›å»ºå®Œæˆ: èŠ‚ç‚¹{node_embeddings.shape}, è¾¹{edge_embeddings.shape}")
            return atlas_data
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºembeddingså¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def setup_retrievers(self, atlas_data: Dict[str, Any], sentence_encoder) -> List:
        """è®¾ç½®è¦æµ‹è¯•çš„retrieveråˆ—è¡¨"""
        print("ğŸ”§ è®¾ç½®Retrieveråˆ—è¡¨...")
        
        retrievers = []
        llm_generator = create_model_client(self.config_loader)
        
        try:
            # 1. SimpleGraphRetriever
            simple_graph = SimpleGraphRetriever(
                llm_generator=llm_generator,
                sentence_encoder=sentence_encoder,
                data=atlas_data
            )
            retrievers.append(simple_graph)
            print("âœ… SimpleGraphRetriever å·²æ·»åŠ ")
            
            # 2. SimpleTextRetriever
            passage_dict = atlas_data.get('text_dict', {})
            simple_text = SimpleTextRetriever(
                passage_dict=passage_dict,
                sentence_encoder=sentence_encoder,
                data=atlas_data
            )
            retrievers.append(simple_text)
            print("âœ… SimpleTextRetriever å·²æ·»åŠ ")
            
            # 3. TogRetriever
            try:
                tog_retriever = TogRetriever(
                    llm_generator=llm_generator,
                    sentence_encoder=sentence_encoder,
                    data=atlas_data
                )
                retrievers.append(tog_retriever)
                print("âœ… TogRetriever å·²æ·»åŠ ")
            except Exception as e:
                print(f"âš ï¸ TogRetriever åˆ›å»ºå¤±è´¥: {e}")
            
            # 4. HippoRAGRetriever (å¦‚æœæ”¯æŒ)
            try:
                # ä¸ºHippoRAGå‡†å¤‡ç‰¹æ®Šæ•°æ®æ ¼å¼
                hippo_data = self._prepare_hipporag_data(atlas_data)
                hipporag = HippoRAGRetriever(
                    llm_generator=llm_generator,
                    sentence_encoder=sentence_encoder,
                    data=hippo_data,
                    logger=self.logger
                )
                retrievers.append(hipporag)
                print("âœ… HippoRAGRetriever å·²æ·»åŠ ")
            except Exception as e:
                print(f"âš ï¸ HippoRAGRetriever åˆ›å»ºå¤±è´¥: {e}")
            
            # 5. HippoRAG2Retriever (å¦‚æœæ”¯æŒ)
            try:
                hipporag2 = HippoRAG2Retriever(
                    llm_generator=llm_generator,
                    sentence_encoder=sentence_encoder,
                    data=atlas_data,
                    logger=self.logger
                )
                retrievers.append(hipporag2)
                print("âœ… HippoRAG2Retriever å·²æ·»åŠ ")
            except Exception as e:
                print(f"âš ï¸ HippoRAG2Retriever åˆ›å»ºå¤±è´¥: {e}")
            
        except Exception as e:
            print(f"âŒ è®¾ç½®Retrieverå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"ğŸ“‹ æ€»å…±è®¾ç½®äº† {len(retrievers)} ä¸ªRetriever")
        return retrievers
    
    def _prepare_hipporag_data(self, atlas_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸ºHippoRAGå‡†å¤‡ç‰¹æ®Šçš„æ•°æ®æ ¼å¼"""
        hippo_data = atlas_data.copy()
        
        # ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰passageç±»å‹å’Œfile_id
        for node_id in atlas_data['node_list']:
            if node_id in hippo_data['KG'].nodes:
                hippo_data['KG'].nodes[node_id]['type'] = 'passage'
                hippo_data['KG'].nodes[node_id]['file_id'] = f"file_{node_id}"
        
        return hippo_data
    
    def run_standard_benchmark(self, dataset_name: str = "musique", num_samples: int = 50):
        """è¿è¡Œæ ‡å‡†benchmarkæµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹æ ‡å‡†benchmarkæµ‹è¯• (æ•°æ®é›†: {dataset_name}, æ ·æœ¬æ•°: {num_samples})")
        print("=" * 80)
        
        try:
            # 1. å‡†å¤‡NewWorkæ•°æ®
            atlas_data, sentence_encoder = self.prepare_newwork_data()
            
            # 2. è®¾ç½®retrievers
            retrievers = self.setup_retrievers(atlas_data, sentence_encoder)
            
            if not retrievers:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„retrieverï¼Œç»ˆæ­¢æµ‹è¯•")
                return
            
            # 3. é…ç½®æ ‡å‡†benchmark
            benchmark_config = BenchMarkConfig(
                dataset_name=dataset_name,
                question_file=f"../benchmark_data/{dataset_name}.json",
                include_concept=True,
                include_events=True,
                reader_model_name=self.config_loader.config_data.get('models', {}).get('qwen_235b', 'qwen'),
                encoder_model_name="all-MiniLM-L6-v2",
                number_of_samples=num_samples,
                react_max_iterations=3
            )
            
            # 4. åˆ›å»ºLLMç”Ÿæˆå™¨
            llm_generator = create_model_client(self.config_loader)
            
            # 5. è¿è¡Œæ ‡å‡†benchmark
            print(f"\nğŸ“Š å¼€å§‹è¿è¡Œæ ‡å‡†RAG Benchmark...")
            print(f"ğŸ“‹ æ•°æ®é›†: {dataset_name}")
            print(f"ğŸ“‹ æ ·æœ¬æ•°: {num_samples}")
            print(f"ğŸ“‹ Retrieveræ•°é‡: {len(retrievers)}")
            print(f"ğŸ“‹ Retrieveråˆ—è¡¨: {[r.__class__.__name__ for r in retrievers]}")
            
            benchmark = RAGBenchmark(config=benchmark_config, logger=self.logger)
            benchmark.run(retrievers, llm_generator)
            
            print(f"\nâœ… æ ‡å‡†benchmarkæµ‹è¯•å®Œæˆ!")
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: ./result/{dataset_name}/")
            
            # 6. æ‰“å°æ‘˜è¦ä¿¡æ¯
            self._print_test_summary(dataset_name, retrievers, num_samples)
            
        except FileNotFoundError as e:
            print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            print(f"ğŸ’¡ è¯·ç¡®ä¿ ../benchmark_data/{dataset_name}.json æ–‡ä»¶å­˜åœ¨")
            print(f"ğŸ’¡ å¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶:")
            benchmark_data_path = Path("../benchmark_data")
            if benchmark_data_path.exists():
                for file in benchmark_data_path.glob("*.json"):
                    print(f"   - {file.name}")
        except Exception as e:
            print(f"âŒ Benchmarkè¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _print_test_summary(self, dataset_name: str, retrievers: List, num_samples: int):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“‹ æµ‹è¯•æ‘˜è¦")
        print("="*60)
        print(f"ğŸ—ƒï¸  æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {num_samples}")
        print(f"ğŸ¤– çŸ¥è¯†å›¾è°±: NewWorkæ¦‚å¿µå›¾è°±")
        print(f"ğŸ“ å›¾è°±è·¯å¾„: {self.newwork_output_path}")
        print(f"ğŸ”§ æµ‹è¯•çš„Retriever:")
        for i, retriever in enumerate(retrievers, 1):
            print(f"   {i}. {retriever.__class__.__name__}")
        print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡: EM, F1, Recall@2, Recall@5")
        print(f"ğŸ’¾ ç»“æœä¿å­˜ä½ç½®: ./result/{dataset_name}/")
        print("="*60)
    
    def run_quick_test(self, num_samples: int = 10):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆå°‘é‡æ ·æœ¬ï¼‰"""
        print(f"âš¡ è¿è¡Œå¿«é€Ÿæµ‹è¯• (æ ·æœ¬æ•°: {num_samples})")
        self.run_standard_benchmark(dataset_name="musique", num_samples=num_samples)
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸ† è¿è¡Œå®Œæ•´è¯„ä¼°")
        
        # å¯ä»¥æµ‹è¯•å¤šä¸ªæ•°æ®é›†
        datasets = [
            ("musique", 200),
            # ("hotpotqa", 100),  # å¦‚æœæœ‰å…¶ä»–æ•°æ®é›†
        ]
        
        for dataset_name, num_samples in datasets:
            print(f"\nğŸ”„ æµ‹è¯•æ•°æ®é›†: {dataset_name}")
            try:
                self.run_standard_benchmark(dataset_name=dataset_name, num_samples=num_samples)
            except Exception as e:
                print(f"âŒ æ•°æ®é›† {dataset_name} æµ‹è¯•å¤±è´¥: {e}")
                continue


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ NewWork KG Standard Benchmark Integration")
    print("=" * 80)
    print("ğŸ¯ ä½¿ç”¨AutoSchemaKGæ ‡å‡†benchmarkè¯„ä¼°NewWorkæ¦‚å¿µå›¾è°±")
    print("=" * 80)
    
    try:
        # åˆ›å»ºbenchmarkå®ä¾‹
        benchmark = NewWorkStandardBenchmark("output/simple_test")
        
        # é€‰æ‹©æµ‹è¯•æ¨¡å¼
        print("\nğŸ“‹ è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. å¿«é€Ÿæµ‹è¯• (10ä¸ªæ ·æœ¬)")
        print("2. æ ‡å‡†æµ‹è¯• (50ä¸ªæ ·æœ¬)") 
        print("3. å®Œæ•´è¯„ä¼° (200ä¸ªæ ·æœ¬)")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3) [é»˜è®¤: 1]: ").strip() or "1"
        
        if choice == "1":
            benchmark.run_quick_test(10)
        elif choice == "2":
            benchmark.run_standard_benchmark(num_samples=50)
        elif choice == "3":
            benchmark.run_full_evaluation()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•")
            benchmark.run_quick_test(10)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†æµ‹è¯•")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()