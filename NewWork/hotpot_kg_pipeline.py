#!/usr/bin/env python3
"""
HotpotQA KGÊûÑÂª∫ÂíåÊµãËØïPipeline - Èò∂ÊÆµ1Âø´ÈÄüÊäÄÊúØÈ™åËØÅ
‰ΩøÁî®HotpotQAËÆ≠ÁªÉÊï∞ÊçÆÊûÑÂª∫KGÔºåÁÑ∂ÂêéÂú®ÂºÄÂèëÈõÜ‰∏äÈ™åËØÅ
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import logging
from datetime import datetime

# Ê∑ªÂä†Ë∑ØÂæÑ
sys.path.append('..')
sys.path.append('.')

from config_loader import ConfigLoader, create_model_client


class HotpotDataProcessor:
    """HotpotQAÊï∞ÊçÆÂ§ÑÁêÜÂô®"""
    
    def __init__(self, hotpot_path: str):
        self.hotpot_path = Path(hotpot_path)
        self.train_file = self.hotpot_path / "hotpot_train_v1.1.json"
        self.dev_file = self.hotpot_path / "hotpot_dev_fullwiki_v1.json"
        
        # Ë∞ÉËØïÔºöÊâìÂç∞ÂÆåÊï¥Ë∑ØÂæÑ
        print(f"üîç Ê£ÄÊü•ËÆ≠ÁªÉÊñá‰ª∂Ë∑ØÂæÑ: {self.train_file}")
        print(f"üîç Ê£ÄÊü•ÂºÄÂèëÊñá‰ª∂Ë∑ØÂæÑ: {self.dev_file}")
        print(f"üîç ËÆ≠ÁªÉÊñá‰ª∂Â≠òÂú®: {self.train_file.exists()}")
        print(f"üîç ÂºÄÂèëÊñá‰ª∂Â≠òÂú®: {self.dev_file.exists()}")
        
    def extract_contexts_for_kg(self, max_samples: int = 5000) -> List[Dict[str, str]]:
        """‰ªéËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÊèêÂèñcontextÁî®‰∫éÊûÑÂª∫KG"""
        print(f"üìä ‰ªéËÆ≠ÁªÉÊï∞ÊçÆÊèêÂèñcontext (ÊúÄÂ§ö{max_samples}‰∏™Ê†∑Êú¨)...")
        
        if not self.train_file.exists():
            raise FileNotFoundError(f"ËÆ≠ÁªÉÊñá‰ª∂‰∏çÂ≠òÂú®: {self.train_file}")
        
        with open(self.train_file, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        # ÈôêÂà∂Ê†∑Êú¨Êï∞Èáè
        if max_samples:
            train_data = train_data[:min(max_samples, 1000)]  # Ëøõ‰∏ÄÊ≠•ÈôêÂà∂Âà∞1000‰∏™Ê†∑Êú¨
            print(f"‚úÖ ‰ΩøÁî®Ââç{min(max_samples, 1000)}‰∏™ËÆ≠ÁªÉÊ†∑Êú¨(ÈôêÂà∂Âà∞1000‰∏™‰ª•ÂÜÖ)")
        
        # ÊèêÂèñÊâÄÊúâcontextÊÆµËêΩ
        contexts = []
        context_id = 0
        
        for sample in train_data:
            for title, paragraphs in sample['context']:
                # Â∞ÜÊØè‰∏™ÊÆµËêΩËΩ¨Êç¢‰∏∫NewWorkÊ†ºÂºè
                full_text = ' '.join(paragraphs)
                
                context_item = {
                    "id": f"hotpot_context_{context_id}",
                    "text": full_text,
                    "metadata": {
                        "lang": "en",
                        "source": "hotpot_train",
                        "title": title,
                        "original_question": sample['question']
                    }
                }
                contexts.append(context_item)
                context_id += 1
        
        print(f"‚úÖ ÊèêÂèñ‰∫Ü{len(contexts)}‰∏™contextÊÆµËêΩ")
        return contexts
    
    def save_contexts_for_newwork(self, contexts: List[Dict], output_file: str):
        """‰øùÂ≠òcontext‰∏∫NewWorkÊ†ºÂºè"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(contexts, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ContextÊï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞: {output_file}")
        return str(output_path)
    
    def prepare_test_data(self, max_test_samples: int = 500) -> List[Dict]:
        """ÂáÜÂ§áÊµãËØïÊï∞ÊçÆ"""
        print(f"üìä ÂáÜÂ§áÊµãËØïÊï∞ÊçÆ (ÊúÄÂ§ö{max_test_samples}‰∏™Ê†∑Êú¨)...")
        
        if not self.dev_file.exists():
            raise FileNotFoundError(f"ÂºÄÂèëÊñá‰ª∂‰∏çÂ≠òÂú®: {self.dev_file}")
        
        with open(self.dev_file, 'r', encoding='utf-8') as f:
            dev_data = json.load(f)
        
        if max_test_samples:
            dev_data = dev_data[:max_test_samples]
            print(f"‚úÖ ‰ΩøÁî®Ââç{max_test_samples}‰∏™ÊµãËØïÊ†∑Êú¨")
        
        return dev_data


class HotpotKGPipeline:
    """HotpotQA KGÊûÑÂª∫ÂíåÊµãËØïPipeline"""
    
    def __init__(self, hotpot_path: str):
        self.hotpot_path = hotpot_path
        self.processor = HotpotDataProcessor(hotpot_path)
        self.config_loader = ConfigLoader()
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        """ËÆæÁΩÆÊó•Âøó"""
        logger = logging.getLogger('HotpotKGPipeline')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def stage1_build_kg_from_hotpot(self, max_samples: int = 5000):
        """Èò∂ÊÆµ1: ‰ªéHotpotQAËÆ≠ÁªÉÊï∞ÊçÆÊûÑÂª∫KG"""
        print("üöÄ Èò∂ÊÆµ1: ‰ªéHotpotQAËÆ≠ÁªÉÊï∞ÊçÆÊûÑÂª∫Áü•ËØÜÂõæË∞±")
        print("=" * 70)
        
        try:
            # 1. ÊèêÂèñtraining context
            contexts = self.processor.extract_contexts_for_kg(max_samples)
            
            # 2. ‰øùÂ≠ò‰∏∫NewWorkÊ†ºÂºè
            context_file = "hotpot_contexts_for_kg.json"
            self.processor.save_contexts_for_newwork(contexts, context_file)
            
            # 3. ‰ΩøÁî®NewWork pipelineÊûÑÂª∫KG
            print("\nüîß ‰ΩøÁî®NewWork pipelineÊûÑÂª∫KG...")
            kg_output_path = self._run_newwork_pipeline(context_file)
            
            print(f"\n‚úÖ Èò∂ÊÆµ1ÂÆåÊàêÔºÅKGÂ∑≤ÊûÑÂª∫ÂÆåÊàê")
            print(f"üìÅ KGËæìÂá∫Ë∑ØÂæÑ: {kg_output_path}")
            
            return kg_output_path
            
        except Exception as e:
            print(f"‚ùå Èò∂ÊÆµ1Â§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _run_newwork_pipeline(self, context_file: str) -> str:
        """ËøêË°åNewWork pipelineÊûÑÂª∫KG"""
        try:
            # ÂØºÂÖ•NewWorkÁªÑ‰ª∂
            from direct_concept_pipeline import DirectConceptPipeline
            from direct_concept_config import DirectConceptConfig
            
            # ÂàõÂª∫LLMÊ®°ÂûãÂÆû‰æã
            model = create_model_client(self.config_loader)
            
            # ÈÖçÁΩÆNewWork pipeline - ÊÄßËÉΩ‰ºòÂåñÁâà
            config = DirectConceptConfig(
                model_path="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",  # ‰ΩøÁî®Llama-3-8B-Turbo
                data_directory=".",
                filename_pattern=context_file.replace('.json', ''),
                output_directory="output",
                extraction_mode="passage_concept",
                language="en",
                batch_size_concept=20,    # 8BÊ®°ÂûãÂèØ‰ª•Êõ¥È´òÂπ∂Âèë  
                max_workers=6,            # 6‰∏™workerÂπ∂Âèë
                temperature=0.1,
                text_chunk_size=4096,     # ÈÄÇÂêà8BÊ®°ÂûãÁöÑcontext window
                chunk_overlap=0,          # ÂèñÊ∂àoverlapÂáèÂ∞ëÂÜó‰Ωô
                debug_mode=True
            )
            
            print("ü§ñ ÂàõÂª∫LLMÊ®°ÂûãÂÆû‰æã...")
            print("üìã ÂêØÂä®NewWorkÊ¶ÇÂøµÊèêÂèñpipeline...")
            
            # ËøêË°åpipeline
            pipeline = DirectConceptPipeline(model, config)
            output_path = pipeline.run_full_pipeline("hotpot_kg")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå NewWork pipelineËøêË°åÂ§±Ë¥•: {e}")
            import traceback
            print("üîç ËØ¶ÁªÜÈîôËØØ‰ø°ÊÅØ:")
            traceback.print_exc()
            
            # Â¶ÇÊûúNewWork pipelineÂ§±Ë¥•ÔºåÂàõÂª∫ÁÆÄÂåñÁâàKG
            print("üîÑ Â∞ùËØïÂàõÂª∫ÁÆÄÂåñÁâàÁü•ËØÜÂõæË∞±...")
            return self._create_simple_kg_fallback(context_file)
    
    def _create_simple_kg_fallback(self, context_file: str) -> str:
        """ÂàõÂª∫ÁÆÄÂåñÁâàKG‰Ωú‰∏∫fallback"""
        import pandas as pd
        import networkx as nx
        import pickle
        from datetime import datetime
        
        print("üîÑ ÂàõÂª∫ÁÆÄÂåñÁâàÁü•ËØÜÂõæË∞±...")
        
        # ËØªÂèñcontextÊï∞ÊçÆ
        with open(context_file, 'r') as f:
            contexts = json.load(f)
        
        # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
        output_dir = Path("output/hotpot_kg")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ÁÆÄÂåñÁöÑÂÆû‰ΩìÂíåÂÖ≥Á≥ªÊèêÂèñ
        concepts = []
        relationships = []
        concept_id = 0
        
        for context in contexts[:1000]:  # ÈôêÂà∂Êï∞ÈáèÈÅøÂÖçËøáÂ§ö
            title = context['metadata']['title']
            text = context['text']
            
            # ÁÆÄÂçïÁöÑÊ¶ÇÂøµÊèêÂèñ - ‰ΩøÁî®Ê†áÈ¢ò‰Ωú‰∏∫‰∏ªË¶ÅÊ¶ÇÂøµ
            concepts.append({
                'id': f"concept_{concept_id}",
                'text': title,
                'type': 'entity',
                'abstraction_level': 'specific',
                'source_doc': context['id']
            })
            
            # ÁÆÄÂçïÁöÑÂÖ≥Á≥ª - Ê†áÈ¢ò‰∏éÊñáÊú¨ÂÜÖÂÆπÁöÑÂÖ≥Á≥ª
            if len(text) > 50:
                relationships.append({
                    'source': f"concept_{concept_id}",
                    'target': f"concept_{concept_id}",
                    'relation': 'described_by',
                    'relation_type': 'concept_relation'
                })
            
            concept_id += 1
        
        # ‰øùÂ≠òCSVÊ†ºÂºè
        concepts_df = pd.DataFrame(concepts)
        relationships_df = pd.DataFrame(relationships)
        
        csv_dir = output_dir / "concept_csv"
        csv_dir.mkdir(exist_ok=True)
        
        concepts_df.to_csv(csv_dir / "concepts_hotpot_kg.csv", index=False)
        relationships_df.to_csv(csv_dir / "relationships_hotpot_kg.csv", index=False)
        
        # ÂàõÂª∫ÁÆÄÂçïÁöÑNetworkXÂõæ
        G = nx.Graph()
        for concept in concepts:
            G.add_node(concept['id'], **concept)
        
        for rel in relationships:
            G.add_edge(rel['source'], rel['target'], relation=rel['relation'])
        
        # ‰øùÂ≠òÂõæ
        graph_dir = output_dir / "graph"
        graph_dir.mkdir(exist_ok=True)
        
        with open(graph_dir / "hotpot_kg.pkl", 'wb') as f:
            pickle.dump(G, f)
        
        # ‰øùÂ≠òÁªüËÆ°‰ø°ÊÅØ
        stats = {
            "total_nodes": len(G.nodes),
            "total_edges": len(G.edges),
            "created_at": datetime.now().isoformat(),
            "data_source": "hotpot_train_simplified"
        }
        
        with open(output_dir / "statistics.json", 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"‚úÖ ÁÆÄÂåñÁâàKGÂàõÂª∫ÂÆåÊàê: {len(G.nodes)}‰∏™ËäÇÁÇπ, {len(G.edges)}Êù°Ëæπ")
        return str(output_dir)
    
    def stage2_test_on_dev_set(self, kg_output_path: str, max_test_samples: int = 100):
        """Èò∂ÊÆµ2: Âú®ÂºÄÂèëÈõÜ‰∏äÊµãËØïKGÊïàÊûú"""
        print("\nüß™ Èò∂ÊÆµ2: Âú®HotpotQAÂºÄÂèëÈõÜ‰∏äÊµãËØïKGÊïàÊûú")
        print("=" * 70)
        
        try:
            # 1. ÂáÜÂ§áÊµãËØïÊï∞ÊçÆ
            test_data = self.processor.prepare_test_data(max_test_samples)
            
            # 2. ÂàõÂª∫ÊµãËØïÈóÆÁ≠îÂØπÊñá‰ª∂
            test_file = self._create_test_qa_file(test_data)
            
            # 3. ‰ΩøÁî®Ê†áÂáÜbenchmarkÊµãËØï
            print("\nüîß ËøêË°åÊ†áÂáÜbenchmarkÊµãËØï...")
            self._run_standard_benchmark(kg_output_path, test_file)
            
            print(f"\n‚úÖ Èò∂ÊÆµ2ÂÆåÊàêÔºÅÊµãËØïÁªìÊûúÂ∑≤‰øùÂ≠ò")
            
        except Exception as e:
            print(f"‚ùå Èò∂ÊÆµ2Â§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
    
    def _create_test_qa_file(self, test_data: List[Dict]) -> str:
        """ÂàõÂª∫ÊµãËØïÈóÆÁ≠îÊñá‰ª∂"""
        test_qa = []
        
        for item in test_data:
            qa_item = {
                "id": item["_id"],
                "question": item["question"],
                "answer": item["answer"],
                "supporting_facts": item.get("supporting_facts", []),
                "paragraphs": []
            }
            
            # Ê∑ªÂä†contextÊÆµËêΩ
            for title, paragraphs in item["context"]:
                for para in paragraphs:
                    qa_item["paragraphs"].append({
                        "title": title,
                        "text": para,
                        "is_supporting": True  # ÁÆÄÂåñÂ§ÑÁêÜ
                    })
            
            test_qa.append(qa_item)
        
        # ‰øùÂ≠òÊµãËØïÊñá‰ª∂
        test_file = "hotpot_test_questions.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_qa, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ÊµãËØïÈóÆÁ≠îÊñá‰ª∂Â∑≤‰øùÂ≠ò: {test_file}")
        return test_file
    
    def _run_standard_benchmark(self, kg_output_path: str, test_file: str):
        """ËøêË°åÊ†áÂáÜbenchmarkÊµãËØï"""
        try:
            print("üîß Â∞ùËØï‰ΩøÁî®ËøõÈò∂RAG benchmark...")
            # ‰ΩøÁî®Áé∞ÊúâÁöÑadvanced_rag_benchmarkËøõË°åÊµãËØï
            self._run_advanced_rag_test(kg_output_path)
            
        except Exception as e:
            print(f"‚ö†Ô∏è ËøõÈò∂benchmarkÊµãËØïÂ§±Ë¥•: {e}")
            print("üîÑ ËøêË°åÁÆÄÂåñÁâàÊµãËØï...")
            self._run_simple_test(kg_output_path, test_file)
    
    def _run_advanced_rag_test(self, kg_output_path):
        """‰ΩøÁî®advanced_rag_benchmarkÊµãËØïKG"""
        try:
            # ‰∏¥Êó∂Â∞ÜKGÊñá‰ª∂Â§çÂà∂Âà∞expected‰ΩçÁΩÆ
            import shutil
            
            # Â§ÑÁêÜÂ≠óÂÖ∏Ê†ºÂºèÁöÑËæìÂá∫Ë∑ØÂæÑ
            if isinstance(kg_output_path, dict):
                source_graph = Path(kg_output_path['pickle_file'])
            else:
                source_graph = Path(kg_output_path) / "graph" / "hotpot_kg.pkl"
            target_dir = Path("output/simple_test")
            target_dir.mkdir(parents=True, exist_ok=True)
            target_graph = target_dir / "dulce_simple.pkl"
            
            if source_graph.exists():
                print(f"üìã Â§çÂà∂KGÊñá‰ª∂: {source_graph} -> {target_graph}")
                shutil.copy2(source_graph, target_graph)
                
                # ËøêË°åadvanced benchmark
                print("üöÄ ÂêØÂä®Advanced RAG Benchmark...")
                
                # Áõ¥Êé•ËøêË°åadvanced_rag_benchmarkÁöÑmainÂáΩÊï∞
                import subprocess
                import sys
                
                result = subprocess.run([
                    sys.executable, 
                    "advanced_rag_benchmark.py"
                ], capture_output=True, text=True, cwd=".")
                
                print("üìã Advanced RAG BenchmarkËæìÂá∫:")
                print(result.stdout)
                if result.stderr:
                    print("‚ö†Ô∏è ÈîôËØØ‰ø°ÊÅØ:")
                    print(result.stderr)
                
                print("‚úÖ Advanced RAGÊµãËØïÂÆåÊàê")
                return result.returncode == 0
            else:
                raise FileNotFoundError(f"Ê∫êKGÊñá‰ª∂‰∏çÂ≠òÂú®: {source_graph}")
                
        except Exception as e:
            print(f"‚ùå Advanced RAGÊµãËØïÂ§±Ë¥•: {e}")
            raise
    
    def _run_simple_test(self, kg_output_path, test_file: str):
        """ËøêË°åÁÆÄÂåñÁâàÊµãËØï"""
        print("üîÑ ËøêË°åÁÆÄÂåñÁâàKGÊµãËØï...")
        
        # Âä†ËΩΩKG - Â§ÑÁêÜÂ≠óÂÖ∏Ê†ºÂºèÁöÑËæìÂá∫Ë∑ØÂæÑ
        if isinstance(kg_output_path, dict):
            graph_file = Path(kg_output_path['pickle_file'])
        else:
            graph_file = Path(kg_output_path) / "graph" / "hotpot_kg.pkl"
        if graph_file.exists():
            import pickle
            with open(graph_file, 'rb') as f:
                kg = pickle.load(f)
            
            print(f"üìä KG‰ø°ÊÅØ: {len(kg.nodes)}‰∏™ËäÇÁÇπ, {len(kg.edges)}Êù°Ëæπ")
        
        # Âä†ËΩΩÊµãËØïÈóÆÈ¢ò
        with open(test_file, 'r') as f:
            test_questions = json.load(f)
        
        print(f"üìä ÊµãËØïÈóÆÈ¢ò: {len(test_questions)}‰∏™")
        
        # ÁÆÄÂçïÁöÑÊ£ÄÁ¥¢ÊµãËØï
        print("üîç ËøêË°åÁÆÄÂçïÊ£ÄÁ¥¢ÊµãËØï...")
        for i, qa in enumerate(test_questions[:5]):  # Âè™ÊµãËØïÂâç5‰∏™
            question = qa['question']
            answer = qa['answer']
            print(f"\nÈóÆÈ¢ò{i+1}: {question}")
            print(f"Á≠îÊ°à: {answer}")
            
            # ÁÆÄÂçïÁöÑÂÖ≥ÈîÆËØçÂåπÈÖç
            keywords = question.lower().split()
            relevant_nodes = []
            
            if 'kg' in locals():
                for node_id, node_data in kg.nodes(data=True):
                    node_text = node_data.get('text', '').lower()
                    if any(keyword in node_text for keyword in keywords[:3]):
                        relevant_nodes.append(node_text)
            
            if relevant_nodes:
                print(f"Áõ∏ÂÖ≥ËäÇÁÇπ: {relevant_nodes[:2]}")
            else:
                print("Êú™ÊâæÂà∞Áõ∏ÂÖ≥ËäÇÁÇπ")
        
        print(f"\n‚úÖ ÁÆÄÂåñÊµãËØïÂÆåÊàê")


def main():
    """‰∏ªÂáΩÊï∞"""
    print("üåü HotpotQA KG Pipeline - Èò∂ÊÆµ1Âø´ÈÄüÊäÄÊúØÈ™åËØÅ")
    print("=" * 80)
    
    # ËØªÂèñHotpotQAË∑ØÂæÑ
    hotpot_location_file = Path("hotpotlocation.txt")
    if not hotpot_location_file.exists():
        print("‚ùå Êâæ‰∏çÂà∞hotpotlocation.txtÊñá‰ª∂")
        return
    
    with open(hotpot_location_file, 'r') as f:
        hotpot_path = f.read().strip()
    
    print(f"üîç ËØªÂèñÂà∞ÁöÑË∑ØÂæÑ: '{hotpot_path}'")
    print(f"üîç Ë∑ØÂæÑÈïøÂ∫¶: {len(hotpot_path)}")
    
    # Á°Æ‰øùË∑ØÂæÑ‰∏ç‰∏∫Á©∫
    if not hotpot_path:
        print("‚ùå hotpotlocation.txt‰∏≠ÁöÑË∑ØÂæÑ‰∏∫Á©∫")
        return
    
    if not Path(hotpot_path).exists():
        print(f"‚ùå HotpotQAË∑ØÂæÑ‰∏çÂ≠òÂú®: {hotpot_path}")
        return
    
    print(f"üìÅ HotpotQAË∑ØÂæÑ: {hotpot_path}")
    
    try:
        # ÂàõÂª∫pipeline
        pipeline = HotpotKGPipeline(hotpot_path)
        
        print("\nüìã Èò∂ÊÆµ1ÈÖçÁΩÆ:")
        print("  - Êï∞ÊçÆÊ∫ê: HotpotQAËÆ≠ÁªÉÈõÜcontext")
        print("  - Ê†∑Êú¨Êï∞: 5000‰∏™contextÊÆµËêΩ")
        print("  - ÊñπÊ≥ï: NewWorkÊ¶ÇÂøµÊèêÂèñpipeline")
        print("  - ËæìÂá∫: Ê¶ÇÂøµÂõæË∞±")
        
        # ËøêË°åÈò∂ÊÆµ1 - ‰ΩøÁî®ËæÉÂ∞èÁöÑÊ†∑Êú¨Êï∞ËøõË°åÂø´ÈÄüÈ™åËØÅ
        kg_output_path = pipeline.stage1_build_kg_from_hotpot(max_samples=100)
        
        # ËøêË°åÈò∂ÊÆµ2  
        pipeline.stage2_test_on_dev_set(kg_output_path, max_test_samples=100)
        
        print("\nüéâ HotpotQA KG PipelineÈò∂ÊÆµ1ÂÆåÊàêÔºÅ")
        print(f"üìä ÁªìÊûú‰øùÂ≠òÂú®: {kg_output_path}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Áî®Êà∑‰∏≠Êñ≠‰∫Üpipeline")
    except Exception as e:
        print(f"\n‚ùå PipelineÊâßË°åÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()