#!/usr/bin/env python3
"""
HotpotQA KGæ„å»ºå’Œæµ‹è¯•Pipeline - é˜¶æ®µ1å¿«é€ŸæŠ€æœ¯éªŒè¯
ä½¿ç”¨HotpotQAè®­ç»ƒæ•°æ®æ„å»ºKGï¼Œç„¶ååœ¨å¼€å‘é›†ä¸ŠéªŒè¯
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import logging
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.append('..')
sys.path.append('.')

from config_loader import ConfigLoader, create_model_client


class HotpotDataProcessor:
    """HotpotQAæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, hotpot_path: str):
        self.hotpot_path = Path(hotpot_path)
        self.train_file = self.hotpot_path / "hotpot_train_v1.1.json"
        self.dev_file = self.hotpot_path / "hotpot_dev_fullwiki_v1.json"
        
        # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´è·¯å¾„
        print(f"ğŸ” æ£€æŸ¥è®­ç»ƒæ–‡ä»¶è·¯å¾„: {self.train_file}")
        print(f"ğŸ” æ£€æŸ¥å¼€å‘æ–‡ä»¶è·¯å¾„: {self.dev_file}")
        print(f"ğŸ” è®­ç»ƒæ–‡ä»¶å­˜åœ¨: {self.train_file.exists()}")
        print(f"ğŸ” å¼€å‘æ–‡ä»¶å­˜åœ¨: {self.dev_file.exists()}")
        
    def extract_contexts_for_kg(self, max_samples: int = 5000) -> List[Dict[str, str]]:
        """ä»è®­ç»ƒæ•°æ®ä¸­æå–contextç”¨äºæ„å»ºKG"""
        print(f"ğŸ“Š ä»è®­ç»ƒæ•°æ®æå–context (æœ€å¤š{max_samples}ä¸ªæ ·æœ¬)...")
        
        if not self.train_file.exists():
            raise FileNotFoundError(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {self.train_file}")
        
        with open(self.train_file, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if max_samples:
            train_data = train_data[:min(max_samples, 1000)]  # è¿›ä¸€æ­¥é™åˆ¶åˆ°1000ä¸ªæ ·æœ¬
            print(f"âœ… ä½¿ç”¨å‰{min(max_samples, 1000)}ä¸ªè®­ç»ƒæ ·æœ¬(é™åˆ¶åˆ°1000ä¸ªä»¥å†…)")
        
        # æå–æ‰€æœ‰contextæ®µè½
        contexts = []
        context_id = 0
        
        for sample in train_data:
            for title, paragraphs in sample['context']:
                # å°†æ¯ä¸ªæ®µè½è½¬æ¢ä¸ºNewWorkæ ¼å¼
                full_text = ' '.join(paragraphs)
                
                context_item = {
                    "id": f"hotpot_context_{context_id}",
                    "text": full_text,
                    "metadata": {
                        "lang": "en",
                        "source": "hotpot_train",
                        "title": title,
                        "original_question": sample['question']
                    }
                }
                contexts.append(context_item)
                context_id += 1
        
        print(f"âœ… æå–äº†{len(contexts)}ä¸ªcontextæ®µè½")
        return contexts
    
    def save_contexts_for_newwork(self, contexts: List[Dict], output_file: str):
        """ä¿å­˜contextä¸ºNewWorkæ ¼å¼"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(contexts, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Contextæ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        return str(output_path)
    
    def prepare_test_data(self, max_test_samples: int = 500) -> List[Dict]:
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ® (æœ€å¤š{max_test_samples}ä¸ªæ ·æœ¬)...")
        
        if not self.dev_file.exists():
            raise FileNotFoundError(f"å¼€å‘æ–‡ä»¶ä¸å­˜åœ¨: {self.dev_file}")
        
        with open(self.dev_file, 'r', encoding='utf-8') as f:
            dev_data = json.load(f)
        
        if max_test_samples:
            dev_data = dev_data[:max_test_samples]
            print(f"âœ… ä½¿ç”¨å‰{max_test_samples}ä¸ªæµ‹è¯•æ ·æœ¬")
        
        return dev_data


class HotpotKGPipeline:
    """HotpotQA KGæ„å»ºå’Œæµ‹è¯•Pipeline"""
    
    def __init__(self, hotpot_path: str):
        self.hotpot_path = hotpot_path
        self.processor = HotpotDataProcessor(hotpot_path)
        self.config_loader = ConfigLoader()
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('HotpotKGPipeline')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def stage1_build_kg_from_hotpot(self, max_samples: int = 5000):
        """é˜¶æ®µ1: ä»HotpotQAè®­ç»ƒæ•°æ®æ„å»ºKG"""
        print("ğŸš€ é˜¶æ®µ1: ä»HotpotQAè®­ç»ƒæ•°æ®æ„å»ºçŸ¥è¯†å›¾è°±")
        print("=" * 70)
        
        try:
            # 1. æå–training context
            contexts = self.processor.extract_contexts_for_kg(max_samples)
            
            # 2. ä¿å­˜ä¸ºNewWorkæ ¼å¼
            context_file = "hotpot_contexts_for_kg.json"
            self.processor.save_contexts_for_newwork(contexts, context_file)
            
            # 3. ä½¿ç”¨NewWork pipelineæ„å»ºKG
            print("\nğŸ”§ ä½¿ç”¨NewWork pipelineæ„å»ºKG...")
            kg_output_path = self._run_newwork_pipeline(context_file)
            
            print(f"\nâœ… é˜¶æ®µ1å®Œæˆï¼KGå·²æ„å»ºå®Œæˆ")
            print(f"ğŸ“ KGè¾“å‡ºè·¯å¾„: {kg_output_path}")
            
            return kg_output_path
            
        except Exception as e:
            print(f"âŒ é˜¶æ®µ1å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _run_newwork_pipeline(self, context_file: str) -> str:
        """è¿è¡ŒNewWork pipelineæ„å»ºKG"""
        try:
            # å¯¼å…¥NewWorkç»„ä»¶
            from direct_concept_pipeline import DirectConceptPipeline
            from direct_concept_config import DirectConceptConfig
            
            # åˆ›å»ºLLMæ¨¡å‹å®ä¾‹
            model = create_model_client(self.config_loader)
            
            # é…ç½®NewWork pipeline - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
            config = DirectConceptConfig(
                model_path="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",  # ä½¿ç”¨Llama-3-8B-Turbo
                data_directory=".",
                filename_pattern=context_file.replace('.json', ''),
                output_directory="output",
                extraction_mode="passage_concept",
                language="en",
                batch_size_concept=20,    # 8Bæ¨¡å‹å¯ä»¥æ›´é«˜å¹¶å‘  
                max_workers=6,            # 6ä¸ªworkerå¹¶å‘
                temperature=0.1,
                text_chunk_size=4096,     # é€‚åˆ8Bæ¨¡å‹çš„context window
                chunk_overlap=0,          # å–æ¶ˆoverlapå‡å°‘å†—ä½™
                debug_mode=True
            )
            
            print("ğŸ¤– åˆ›å»ºLLMæ¨¡å‹å®ä¾‹...")
            print("ğŸ“‹ å¯åŠ¨NewWorkæ¦‚å¿µæå–pipeline...")
            
            # è¿è¡Œpipeline
            pipeline = DirectConceptPipeline(model, config)
            output_path = pipeline.run_full_pipeline("hotpot_kg")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ NewWork pipelineè¿è¡Œå¤±è´¥: {e}")
            import traceback
            print("ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # å¦‚æœNewWork pipelineå¤±è´¥ï¼Œåˆ›å»ºç®€åŒ–ç‰ˆKG
            print("ğŸ”„ å°è¯•åˆ›å»ºç®€åŒ–ç‰ˆçŸ¥è¯†å›¾è°±...")
            return self._create_simple_kg_fallback(context_file)
    
    def _create_simple_kg_fallback(self, context_file: str) -> str:
        """åˆ›å»ºç®€åŒ–ç‰ˆKGä½œä¸ºfallback"""
        import pandas as pd
        import networkx as nx
        import pickle
        from datetime import datetime
        
        print("ğŸ”„ åˆ›å»ºç®€åŒ–ç‰ˆçŸ¥è¯†å›¾è°±...")
        
        # è¯»å–contextæ•°æ®
        with open(context_file, 'r') as f:
            contexts = json.load(f)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("output/hotpot_kg")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç®€åŒ–çš„å®ä½“å’Œå…³ç³»æå–
        concepts = []
        relationships = []
        concept_id = 0
        
        for context in contexts[:1000]:  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            title = context['metadata']['title']
            text = context['text']
            
            # ç®€å•çš„æ¦‚å¿µæå– - ä½¿ç”¨æ ‡é¢˜ä½œä¸ºä¸»è¦æ¦‚å¿µ
            concepts.append({
                'id': f"concept_{concept_id}",
                'text': title,
                'type': 'entity',
                'abstraction_level': 'specific',
                'source_doc': context['id']
            })
            
            # ç®€å•çš„å…³ç³» - æ ‡é¢˜ä¸æ–‡æœ¬å†…å®¹çš„å…³ç³»
            if len(text) > 50:
                relationships.append({
                    'source': f"concept_{concept_id}",
                    'target': f"concept_{concept_id}",
                    'relation': 'described_by',
                    'relation_type': 'concept_relation'
                })
            
            concept_id += 1
        
        # ä¿å­˜CSVæ ¼å¼
        concepts_df = pd.DataFrame(concepts)
        relationships_df = pd.DataFrame(relationships)
        
        csv_dir = output_dir / "concept_csv"
        csv_dir.mkdir(exist_ok=True)
        
        concepts_df.to_csv(csv_dir / "concepts_hotpot_kg.csv", index=False)
        relationships_df.to_csv(csv_dir / "relationships_hotpot_kg.csv", index=False)
        
        # åˆ›å»ºç®€å•çš„NetworkXå›¾
        G = nx.Graph()
        for concept in concepts:
            G.add_node(concept['id'], **concept)
        
        for rel in relationships:
            G.add_edge(rel['source'], rel['target'], relation=rel['relation'])
        
        # ä¿å­˜å›¾
        graph_dir = output_dir / "graph"
        graph_dir.mkdir(exist_ok=True)
        
        with open(graph_dir / "hotpot_kg.pkl", 'wb') as f:
            pickle.dump(G, f)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_nodes": len(G.nodes),
            "total_edges": len(G.edges),
            "created_at": datetime.now().isoformat(),
            "data_source": "hotpot_train_simplified"
        }
        
        with open(output_dir / "statistics.json", 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"âœ… ç®€åŒ–ç‰ˆKGåˆ›å»ºå®Œæˆ: {len(G.nodes)}ä¸ªèŠ‚ç‚¹, {len(G.edges)}æ¡è¾¹")
        return str(output_dir)
    
    def stage2_test_on_dev_set(self, kg_output_path: str, max_test_samples: int = 100):
        """é˜¶æ®µ2: åœ¨å¼€å‘é›†ä¸Šæµ‹è¯•KGæ•ˆæœ"""
        print("\nğŸ§ª é˜¶æ®µ2: åœ¨HotpotQAå¼€å‘é›†ä¸Šæµ‹è¯•KGæ•ˆæœ")
        print("=" * 70)
        
        try:
            # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
            test_data = self.processor.prepare_test_data(max_test_samples)
            
            # 2. åˆ›å»ºæµ‹è¯•é—®ç­”å¯¹æ–‡ä»¶
            test_file = self._create_test_qa_file(test_data)
            
            # 3. ä½¿ç”¨æ ‡å‡†benchmarkæµ‹è¯•
            print("\nğŸ”§ è¿è¡Œæ ‡å‡†benchmarkæµ‹è¯•...")
            self._run_standard_benchmark(kg_output_path, test_file)
            
            print(f"\nâœ… é˜¶æ®µ2å®Œæˆï¼æµ‹è¯•ç»“æœå·²ä¿å­˜")
            
        except Exception as e:
            print(f"âŒ é˜¶æ®µ2å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _create_test_qa_file(self, test_data: List[Dict]) -> str:
        """åˆ›å»ºæµ‹è¯•é—®ç­”æ–‡ä»¶"""
        test_qa = []
        
        for item in test_data:
            qa_item = {
                "id": item["_id"],
                "question": item["question"],
                "answer": item["answer"],
                "supporting_facts": item.get("supporting_facts", []),
                "paragraphs": []
            }
            
            # æ·»åŠ contextæ®µè½
            for title, paragraphs in item["context"]:
                for para in paragraphs:
                    qa_item["paragraphs"].append({
                        "title": title,
                        "text": para,
                        "is_supporting": True  # ç®€åŒ–å¤„ç†
                    })
            
            test_qa.append(qa_item)
        
        # ä¿å­˜æµ‹è¯•æ–‡ä»¶
        test_file = "hotpot_test_questions.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_qa, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æµ‹è¯•é—®ç­”æ–‡ä»¶å·²ä¿å­˜: {test_file}")
        return test_file
    
    def _run_standard_benchmark(self, kg_output_path: str, test_file: str):
        """è¿è¡Œæ ‡å‡†benchmarkæµ‹è¯•"""
        try:
            print("ğŸ”§ å°è¯•ä½¿ç”¨è¿›é˜¶RAG benchmark...")
            # ä½¿ç”¨ç°æœ‰çš„advanced_rag_benchmarkè¿›è¡Œæµ‹è¯•
            self._run_advanced_rag_test(kg_output_path)
            
        except Exception as e:
            print(f"âš ï¸ è¿›é˜¶benchmarkæµ‹è¯•å¤±è´¥: {e}")
            print("ğŸ”„ è¿è¡Œç®€åŒ–ç‰ˆæµ‹è¯•...")
            self._run_simple_test(kg_output_path, test_file)
    
    def _run_advanced_rag_test(self, kg_output_path):
        """ä½¿ç”¨advanced_rag_benchmarkæµ‹è¯•KG"""
        try:
            # ä¸´æ—¶å°†KGæ–‡ä»¶å¤åˆ¶åˆ°expectedä½ç½®
            import shutil
            
            # å¤„ç†å­—å…¸æ ¼å¼çš„è¾“å‡ºè·¯å¾„
            if isinstance(kg_output_path, dict):
                source_graph = Path(kg_output_path['pickle_file'])
            else:
                source_graph = Path(kg_output_path) / "graph" / "hotpot_kg.pkl"
            target_dir = Path("output/simple_test")
            target_dir.mkdir(parents=True, exist_ok=True)
            target_graph = target_dir / "dulce_simple.pkl"
            
            if source_graph.exists():
                print(f"ğŸ“‹ å¤åˆ¶KGæ–‡ä»¶: {source_graph} -> {target_graph}")
                shutil.copy2(source_graph, target_graph)
                
                # è¿è¡Œadvanced benchmark
                print("ğŸš€ å¯åŠ¨Advanced RAG Benchmark...")
                
                # ç›´æ¥è¿è¡Œadvanced_rag_benchmarkçš„mainå‡½æ•°
                import subprocess
                import sys
                
                result = subprocess.run([
                    sys.executable, 
                    "advanced_rag_benchmark.py"
                ], capture_output=True, text=True, cwd=".")
                
                print("ğŸ“‹ Advanced RAG Benchmarkè¾“å‡º:")
                print(result.stdout)
                if result.stderr:
                    print("âš ï¸ é”™è¯¯ä¿¡æ¯:")
                    print(result.stderr)
                
                print("âœ… Advanced RAGæµ‹è¯•å®Œæˆ")
                return result.returncode == 0
            else:
                raise FileNotFoundError(f"æºKGæ–‡ä»¶ä¸å­˜åœ¨: {source_graph}")
                
        except Exception as e:
            print(f"âŒ Advanced RAGæµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def _run_simple_test(self, kg_output_path, test_file: str):
        """è¿è¡Œç®€åŒ–ç‰ˆæµ‹è¯•"""
        print("ğŸ”„ è¿è¡Œç®€åŒ–ç‰ˆKGæµ‹è¯•...")
        
        # åŠ è½½KG - å¤„ç†å­—å…¸æ ¼å¼çš„è¾“å‡ºè·¯å¾„
        if isinstance(kg_output_path, dict):
            graph_file = Path(kg_output_path['pickle_file'])
        else:
            graph_file = Path(kg_output_path) / "graph" / "hotpot_kg.pkl"
        if graph_file.exists():
            import pickle
            with open(graph_file, 'rb') as f:
                kg = pickle.load(f)
            
            print(f"ğŸ“Š KGä¿¡æ¯: {len(kg.nodes)}ä¸ªèŠ‚ç‚¹, {len(kg.edges)}æ¡è¾¹")
        
        # åŠ è½½æµ‹è¯•é—®é¢˜
        with open(test_file, 'r') as f:
            test_questions = json.load(f)
        
        print(f"ğŸ“Š æµ‹è¯•é—®é¢˜: {len(test_questions)}ä¸ª")
        
        # ç®€å•çš„æ£€ç´¢æµ‹è¯•
        print("ğŸ” è¿è¡Œç®€å•æ£€ç´¢æµ‹è¯•...")
        for i, qa in enumerate(test_questions[:5]):  # åªæµ‹è¯•å‰5ä¸ª
            question = qa['question']
            answer = qa['answer']
            print(f"\né—®é¢˜{i+1}: {question}")
            print(f"ç­”æ¡ˆ: {answer}")
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            keywords = question.lower().split()
            relevant_nodes = []
            
            if 'kg' in locals():
                for node_id, node_data in kg.nodes(data=True):
                    node_text = node_data.get('text', '').lower()
                    if any(keyword in node_text for keyword in keywords[:3]):
                        relevant_nodes.append(node_text)
            
            if relevant_nodes:
                print(f"ç›¸å…³èŠ‚ç‚¹: {relevant_nodes[:2]}")
            else:
                print("æœªæ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹")
        
        print(f"\nâœ… ç®€åŒ–æµ‹è¯•å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ HotpotQA KG Pipeline - é˜¶æ®µ1å¿«é€ŸæŠ€æœ¯éªŒè¯")
    print("=" * 80)
    
    # è¯»å–HotpotQAè·¯å¾„
    hotpot_location_file = Path("hotpotlocation.txt")
    if not hotpot_location_file.exists():
        print("âŒ æ‰¾ä¸åˆ°hotpotlocation.txtæ–‡ä»¶")
        return
    
    with open(hotpot_location_file, 'r') as f:
        hotpot_path = f.read().strip()
    
    print(f"ğŸ” è¯»å–åˆ°çš„è·¯å¾„: '{hotpot_path}'")
    print(f"ğŸ” è·¯å¾„é•¿åº¦: {len(hotpot_path)}")
    
    # ç¡®ä¿è·¯å¾„ä¸ä¸ºç©º
    if not hotpot_path:
        print("âŒ hotpotlocation.txtä¸­çš„è·¯å¾„ä¸ºç©º")
        return
    
    if not Path(hotpot_path).exists():
        print(f"âŒ HotpotQAè·¯å¾„ä¸å­˜åœ¨: {hotpot_path}")
        return
    
    print(f"ğŸ“ HotpotQAè·¯å¾„: {hotpot_path}")
    
    try:
        # åˆ›å»ºpipeline
        pipeline = HotpotKGPipeline(hotpot_path)
        
        print("\nğŸ“‹ é˜¶æ®µ1é…ç½®:")
        print("  - æ•°æ®æº: HotpotQAè®­ç»ƒé›†context")
        print("  - æ ·æœ¬æ•°: 5000ä¸ªcontextæ®µè½")
        print("  - æ–¹æ³•: NewWorkæ¦‚å¿µæå–pipeline")
        print("  - è¾“å‡º: æ¦‚å¿µå›¾è°±")
        
        # è¿è¡Œé˜¶æ®µ1 - ä½¿ç”¨è¾ƒå°çš„æ ·æœ¬æ•°è¿›è¡Œå¿«é€ŸéªŒè¯
        kg_output_path = pipeline.stage1_build_kg_from_hotpot(max_samples=100)
        
        # è¿è¡Œé˜¶æ®µ2  
        pipeline.stage2_test_on_dev_set(kg_output_path, max_test_samples=100)
        
        print("\nğŸ‰ HotpotQA KG Pipelineé˜¶æ®µ1å®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {kg_output_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†pipeline")
    except Exception as e:
        print(f"\nâŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()